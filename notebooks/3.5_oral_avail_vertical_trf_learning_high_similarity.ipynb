{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pretrained Vertical Model for oral bioavailability prediction\n",
    "\n",
    "1. This notebook used the verticalGNN model pretrained with high similarity solubility dataset\n",
    "2. Ensure that the notebook, data folder, config.py, engine.py, model.py and utils.py are in the same directory. \n",
    "3. Whole process can be repeated by running the cells in order\n",
    "4. Otherwise, download the saved models as described in the README.md from google drive, comment the run_training line to get the results in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required materials\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "from model import VerticalGNN\n",
    "from config import NUM_FEATURES, NUM_TARGET, EDGE_DIM, DEVICE, SEED_NO, PATIENCE, EPOCHS, NUM_GRAPHS_PER_BATCH, N_SPLITS, best_params_vertical\n",
    "from engine import EngineSol, EngineHOB\n",
    "from utils import seed_everything, LoadHOBDataset, LoadSolDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a train and test function that can aid us in transfer learning. Then, we evaluate how different factors can affect the results of transfer learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(method_tf, train_loader, valid_loader, params,es_trigger, path_to_pretrained_model, path_to_save_trained_model):\n",
    "    \n",
    "    '''\n",
    "    Define a function to wrap training\n",
    "\n",
    "    Args:\n",
    "    method_tf (str): freeze --> freeze parameters of feature extraction block, fine_tune_5x -> fine tune at 5x slower learning rate, \n",
    "                    fine_tune_10x -> fine tune at 10x slower learning rate\n",
    "    train_loader: DataLoader class from pytorch geometric containing train data\n",
    "    valid_loader: DataLoader class from pytorch geometric containing validation data\n",
    "    params (dict): dictionary containing the hyperparameters\n",
    "    es_trigger (int): a number to force train model before triggering early stopping mechanism \n",
    "    path_to_pretrained_model (str): path to load the pretrained models\n",
    "    path_to_save_trained_model: path to save the trained models\n",
    "\n",
    "    Return:\n",
    "    best loss: return best validation loss\n",
    "    '''\n",
    "    \n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_pretrained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])\n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "    eng = EngineHOB(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss, acc_score, f1, roc_auc = eng.validate(valid_loader)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}\"\n",
    "        )\n",
    "        if epoch+1>es_trigger:\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                early_stopping_counter = 0  # reset counter\n",
    "                print(\"Saving model...\")\n",
    "                torch.save(model.state_dict(), path_to_save_trained_model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter > early_stopping_iter:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "            print(f\"Early stop counter: {early_stopping_counter}\")\n",
    "\n",
    "    return best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(method_tf, valid_loader, params, path_to_trained_model):\n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_trained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])        \n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "\n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    eng = EngineHOB(model, optimizer, device=DEVICE)\n",
    "    bce, acc, f1, roc_auc = eng.validate(valid_loader)\n",
    "    print(f\"bce:{bce}, acc :{acc}, f1: {f1}, roc_auc: {roc_auc}\")\n",
    "    return bce, acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(method_tf, test_loader, params, path_to_trained_model):\n",
    "    \n",
    "    '''\n",
    "    Define a function to wrap testing\n",
    "\n",
    "    Args:\n",
    "    method_tf (str): freeze --> freeze parameters of feature extraction block, fine_tune_5x -> fine tune at 5x slower learning rate, \n",
    "                    fine_tune_10x -> fine tune at 10x slower learning rate\n",
    "    test_loader: DataLoader class from pytorch geometric containing test data\n",
    "    params (dict): dictionary containing the hyperparameters\n",
    "    path_to_save_trained_model: path to load the saved trained models\n",
    "\n",
    "    Return:\n",
    "    bce: logloss from pytorch geometric \n",
    "    acc: accuracy score\n",
    "    f1: f1 score\n",
    "    roc_auc: roc auc score\n",
    "    '''\n",
    "    \n",
    "    model = VerticalGNN(\n",
    "            num_features=NUM_FEATURES,\n",
    "            num_targets=NUM_TARGET,\n",
    "            num_gin_layers=params[\"num_gin_layers\"],\n",
    "            num_graph_trans_layers=params[\"num_graph_trans_layers\"],\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            n_heads=params[\"n_heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            edge_dim=EDGE_DIM,\n",
    "        )\n",
    "\n",
    "    model.load_state_dict(torch.load(path_to_trained_model))  \n",
    "    model.to(DEVICE)\n",
    "    if method_tf == 'freeze':\n",
    "        for param in model.gin_model.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in model.graph_trans_model.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = params['learning_rate'])        \n",
    "    \n",
    "    elif method_tf == 'fine_tune_5x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/5},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    elif method_tf == 'fine_tune_10x':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']/10},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "    elif method_tf == 'fine_tune':\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params': model.gin_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.graph_trans_model.parameters(), 'lr': params['learning_rate']},\n",
    "            {'params': model.ro.parameters()}\n",
    "        ],lr = params['learning_rate'])\n",
    "        \n",
    "\n",
    "    eng = EngineHOB(model, optimizer, device=DEVICE)\n",
    "    bce, acc, f1, roc_auc = eng.test(test_loader)\n",
    "    print(f\"bce:{bce}, acc :{acc}, f1: {f1}, roc_auc: {roc_auc}\")\n",
    "    return bce, acc, f1, roc_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of number of pre-training epochs to transfer learning prediction performance\n",
    "1. Weights are first frozen for the feature extraction block. \n",
    "2. Models are allowed to train and only weights for the classifier block is allowed to be updated. \n",
    "3. To evaluate how number of pre-training epochs affect the transfer learning prediction performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on path\n",
    "1. path_to_pretrained_model = path to model pretrained with solubility dataset\n",
    "2. path_to_save_trained_model = path to pretrained model trained with oral bioavailability dataset\n",
    "3. path_to_trained_model = path to pretrained model trained with oral bioavailability dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5906937718391418, acc :0.6939655172413793, f1: 0.8022284122562675, roc_auc: 0.6828609986504723\n",
      "bce:0.5707521140575409, acc :0.6968060661764706, f1: 0.7883715622503709, roc_auc: 0.781543219043219\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6356474757194519, acc :0.6508620689655172, f1: 0.7839999999999999, roc_auc: 0.6134453781512605\n",
      "bce:0.5464897751808167, acc :0.7232306985294117, f1: 0.8041850869437077, roc_auc: 0.8015584265584266\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6616944670677185, acc :0.5800865800865801, f1: 0.616600790513834, roc_auc: 0.6313184340782961\n",
      "bce:0.518362894654274, acc :0.7877987132352942, f1: 0.8298730834128091, roc_auc: 0.8063492750992751\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6838578581809998, acc :0.5281385281385281, f1: 0.5112107623318385, roc_auc: 0.5695760211212921\n",
      "bce:0.5375787913799286, acc :0.7594209558823529, f1: 0.8157467532467533, roc_auc: 0.8098056848056848\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6146137714385986, acc :0.70995670995671, f1: 0.6968325791855203, roc_auc: 0.7313813813813814\n",
      "bce:0.5360859483480453, acc :0.7622931985294117, f1: 0.811128015648772, roc_auc: 0.811690467940468\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5906937718391418, acc :0.6939655172413793, f1: 0.8022284122562675, roc_auc: 0.6828609986504723\n",
      "bce:0.5707521140575409, acc :0.6968060661764706, f1: 0.7883715622503709, roc_auc: 0.781543219043219\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6356474757194519, acc :0.6508620689655172, f1: 0.7839999999999999, roc_auc: 0.6134453781512605\n",
      "bce:0.5464897751808167, acc :0.7232306985294117, f1: 0.8041850869437077, roc_auc: 0.8015584265584266\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6615689992904663, acc :0.5670995670995671, f1: 0.6, roc_auc: 0.6310934453277337\n",
      "bce:0.5194980055093765, acc :0.7877987132352942, f1: 0.8284419347648413, roc_auc: 0.8032352407352408\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6838578581809998, acc :0.5281385281385281, f1: 0.5112107623318385, roc_auc: 0.5695760211212921\n",
      "bce:0.5375788062810898, acc :0.7594209558823529, f1: 0.8157467532467533, roc_auc: 0.8098056848056848\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6146137714385986, acc :0.70995670995671, f1: 0.6968325791855203, roc_auc: 0.7313813813813814\n",
      "bce:0.536085918545723, acc :0.7622931985294117, f1: 0.811128015648772, roc_auc: 0.811690467940468\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5906937718391418, acc :0.6939655172413793, f1: 0.8022284122562675, roc_auc: 0.6828609986504723\n",
      "bce:0.5707521140575409, acc :0.6968060661764706, f1: 0.7883715622503709, roc_auc: 0.781543219043219\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6356474757194519, acc :0.6508620689655172, f1: 0.7839999999999999, roc_auc: 0.6134453781512605\n",
      "bce:0.5464897751808167, acc :0.7232306985294117, f1: 0.8041850869437077, roc_auc: 0.8015584265584266\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6616944670677185, acc :0.5800865800865801, f1: 0.616600790513834, roc_auc: 0.6313184340782961\n",
      "bce:0.518362894654274, acc :0.7877987132352942, f1: 0.8298730834128091, roc_auc: 0.8063492750992751\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6838579177856445, acc :0.5281385281385281, f1: 0.5112107623318385, roc_auc: 0.5695760211212921\n",
      "bce:0.5375787764787674, acc :0.7594209558823529, f1: 0.8157467532467533, roc_auc: 0.8098056848056848\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6146137714385986, acc :0.70995670995671, f1: 0.6968325791855203, roc_auc: 0.7313813813813814\n",
      "bce:0.5360859483480453, acc :0.7622931985294117, f1: 0.811128015648772, roc_auc: 0.811690467940468\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5906937718391418, acc :0.6939655172413793, f1: 0.8022284122562675, roc_auc: 0.6828609986504723\n",
      "bce:0.5707521140575409, acc :0.6968060661764706, f1: 0.7883715622503709, roc_auc: 0.781543219043219\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6356474757194519, acc :0.6508620689655172, f1: 0.7839999999999999, roc_auc: 0.6134453781512605\n",
      "bce:0.5464897751808167, acc :0.7232306985294117, f1: 0.8041850869437077, roc_auc: 0.8015584265584266\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6616944670677185, acc :0.5800865800865801, f1: 0.616600790513834, roc_auc: 0.6313184340782961\n",
      "bce:0.5183629095554352, acc :0.7877987132352942, f1: 0.8298730834128091, roc_auc: 0.8063492750992751\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6838578581809998, acc :0.5281385281385281, f1: 0.5112107623318385, roc_auc: 0.5695760211212921\n",
      "bce:0.5375787764787674, acc :0.7594209558823529, f1: 0.8157467532467533, roc_auc: 0.8098056848056848\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6146137714385986, acc :0.70995670995671, f1: 0.6968325791855203, roc_auc: 0.7313813813813814\n",
      "bce:0.5360859483480453, acc :0.7622931985294117, f1: 0.811128015648772, roc_auc: 0.811690467940468\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5906937718391418, acc :0.6939655172413793, f1: 0.8022284122562675, roc_auc: 0.6828609986504723\n",
      "bce:0.5707521140575409, acc :0.6968060661764706, f1: 0.7883715622503709, roc_auc: 0.781543219043219\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6356474757194519, acc :0.6508620689655172, f1: 0.7839999999999999, roc_auc: 0.6134453781512605\n",
      "bce:0.5464897751808167, acc :0.7232306985294117, f1: 0.8041850869437077, roc_auc: 0.8015584265584266\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6616944670677185, acc :0.5800865800865801, f1: 0.616600790513834, roc_auc: 0.6313184340782961\n",
      "bce:0.518362894654274, acc :0.7877987132352942, f1: 0.8298730834128091, roc_auc: 0.8063492750992751\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6838579177856445, acc :0.5281385281385281, f1: 0.5112107623318385, roc_auc: 0.5695760211212921\n",
      "bce:0.5375787764787674, acc :0.7594209558823529, f1: 0.8157467532467533, roc_auc: 0.8098056848056848\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6146137714385986, acc :0.70995670995671, f1: 0.6968325791855203, roc_auc: 0.7313813813813814\n",
      "bce:0.536085918545723, acc :0.7622931985294117, f1: 0.811128015648772, roc_auc: 0.811690467940468\n",
      "validation bce:0.637±0.033\n",
      "validation acc:0.632±0.069\n",
      "validation f1: 0.682±0.109\n",
      "validation roc_auc: 0.646±0.056\n",
      "bce:0.542±0.017\n",
      "acc:0.746±0.032\n",
      "f1: 0.810±0.014\n",
      "roc_auc: 0.802±0.011\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'freeze'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_20/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_20/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_20_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "\n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.6155871748924255, acc :0.6982758620689655, f1: 0.8148148148148149, roc_auc: 0.5791160593792173\n",
      "bce:0.6866039335727692, acc :0.5900735294117647, f1: 0.7311348781937017, roc_auc: 0.5156488906488906\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6172770261764526, acc :0.6551724137931034, f1: 0.7647058823529411, roc_auc: 0.660984393757503\n",
      "bce:0.4844960421323776, acc :0.7877987132352942, f1: 0.8378378378378378, roc_auc: 0.844056969056969\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6713252663612366, acc :0.5800865800865801, f1: 0.6196078431372549, roc_auc: 0.5964451777411129\n",
      "bce:0.5272093862295151, acc :0.7711397058823529, f1: 0.8156891495601173, roc_auc: 0.8168377855877855\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.686718761920929, acc :0.5670995670995671, f1: 0.5283018867924528, roc_auc: 0.5704301910234508\n",
      "bce:0.5336512625217438, acc :0.7760799632352942, f1: 0.8233989266547406, roc_auc: 0.7967239217239217\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6102309226989746, acc :0.683982683982684, f1: 0.6696832579185521, roc_auc: 0.7239489489489489\n",
      "bce:0.5460249930620193, acc :0.7544806985294117, f1: 0.8076624019226692, roc_auc: 0.8095289657789658\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.6155871748924255, acc :0.6982758620689655, f1: 0.8148148148148149, roc_auc: 0.5791160593792173\n",
      "bce:0.6866039335727692, acc :0.5900735294117647, f1: 0.7311348781937017, roc_auc: 0.5156488906488906\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6117255687713623, acc :0.6508620689655172, f1: 0.7582089552238805, roc_auc: 0.6694677871148459\n",
      "bce:0.4898369759321213, acc :0.7702205882352942, f1: 0.8253968253968254, roc_auc: 0.8280003905003905\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6713252663612366, acc :0.5800865800865801, f1: 0.6196078431372549, roc_auc: 0.5964451777411129\n",
      "bce:0.5272093713283539, acc :0.7711397058823529, f1: 0.8156891495601173, roc_auc: 0.8168377855877855\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6867187023162842, acc :0.5670995670995671, f1: 0.5283018867924528, roc_auc: 0.5704301910234508\n",
      "bce:0.533651277422905, acc :0.7760799632352942, f1: 0.8233989266547406, roc_auc: 0.7967239217239217\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6102309226989746, acc :0.683982683982684, f1: 0.6696832579185521, roc_auc: 0.7239489489489489\n",
      "bce:0.5460249930620193, acc :0.7544806985294117, f1: 0.8076624019226692, roc_auc: 0.8095289657789658\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.6155871748924255, acc :0.6982758620689655, f1: 0.8148148148148149, roc_auc: 0.5791160593792173\n",
      "bce:0.6866039335727692, acc :0.5900735294117647, f1: 0.7311348781937017, roc_auc: 0.5156488906488906\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6172770261764526, acc :0.6551724137931034, f1: 0.7647058823529411, roc_auc: 0.660984393757503\n",
      "bce:0.48449602723121643, acc :0.7877987132352942, f1: 0.8378378378378378, roc_auc: 0.844056969056969\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6713253259658813, acc :0.5800865800865801, f1: 0.6196078431372549, roc_auc: 0.5964451777411129\n",
      "bce:0.5272094160318375, acc :0.7711397058823529, f1: 0.8156891495601173, roc_auc: 0.8168377855877855\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.686718761920929, acc :0.5670995670995671, f1: 0.5283018867924528, roc_auc: 0.5704301910234508\n",
      "bce:0.5336512625217438, acc :0.7760799632352942, f1: 0.8233989266547406, roc_auc: 0.7967239217239217\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6102309823036194, acc :0.683982683982684, f1: 0.6696832579185521, roc_auc: 0.7239489489489489\n",
      "bce:0.5460249930620193, acc :0.7544806985294117, f1: 0.8076624019226692, roc_auc: 0.8095289657789658\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.6155871748924255, acc :0.6982758620689655, f1: 0.8148148148148149, roc_auc: 0.5791160593792173\n",
      "bce:0.6866039335727692, acc :0.5900735294117647, f1: 0.7311348781937017, roc_auc: 0.5156488906488906\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6172770261764526, acc :0.6551724137931034, f1: 0.7647058823529411, roc_auc: 0.660984393757503\n",
      "bce:0.4844960421323776, acc :0.7877987132352942, f1: 0.8378378378378378, roc_auc: 0.844056969056969\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6713253259658813, acc :0.5800865800865801, f1: 0.6196078431372549, roc_auc: 0.5964451777411129\n",
      "bce:0.5272093862295151, acc :0.7711397058823529, f1: 0.8156891495601173, roc_auc: 0.8168377855877855\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.686718761920929, acc :0.5670995670995671, f1: 0.5283018867924528, roc_auc: 0.5704301910234508\n",
      "bce:0.5336512625217438, acc :0.7760799632352942, f1: 0.8233989266547406, roc_auc: 0.7967239217239217\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6102309823036194, acc :0.683982683982684, f1: 0.6696832579185521, roc_auc: 0.7239489489489489\n",
      "bce:0.5460249930620193, acc :0.7544806985294117, f1: 0.8076624019226692, roc_auc: 0.8095289657789658\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.6155871748924255, acc :0.6982758620689655, f1: 0.8148148148148149, roc_auc: 0.5791160593792173\n",
      "bce:0.6866039335727692, acc :0.5900735294117647, f1: 0.7311348781937017, roc_auc: 0.5156488906488906\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6117255687713623, acc :0.6508620689655172, f1: 0.7582089552238805, roc_auc: 0.6694677871148459\n",
      "bce:0.4898369461297989, acc :0.7702205882352942, f1: 0.8253968253968254, roc_auc: 0.8280003905003905\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6713252663612366, acc :0.5800865800865801, f1: 0.6196078431372549, roc_auc: 0.5964451777411129\n",
      "bce:0.5272094160318375, acc :0.7711397058823529, f1: 0.8156891495601173, roc_auc: 0.8168377855877855\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.686718761920929, acc :0.5670995670995671, f1: 0.5283018867924528, roc_auc: 0.5704301910234508\n",
      "bce:0.5336512625217438, acc :0.7760799632352942, f1: 0.8233989266547406, roc_auc: 0.7967239217239217\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6102309823036194, acc :0.683982683982684, f1: 0.6696832579185521, roc_auc: 0.7239489489489489\n",
      "bce:0.5460249930620193, acc :0.7544806985294117, f1: 0.8076624019226692, roc_auc: 0.8095289657789658\n",
      "validation bce:0.640±0.032\n",
      "validation acc:0.637±0.054\n",
      "validation f1: 0.679±0.102\n",
      "validation roc_auc: 0.627±0.059\n",
      "bce:0.556±0.068\n",
      "acc:0.735±0.073\n",
      "f1: 0.802±0.037\n",
      "roc_auc: 0.755±0.121\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'freeze'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_40/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_40/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_40_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "\n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.6056687831878662, acc :0.7155172413793104, f1: 0.8124999999999999, roc_auc: 0.6624493927125507\n",
      "bce:0.5857762098312378, acc :0.7163373161764706, f1: 0.7822835185947578, roc_auc: 0.7545443795443795\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6111372709274292, acc :0.6767241379310345, f1: 0.7800586510263929, roc_auc: 0.6579431772709085\n",
      "bce:0.4837230294942856, acc :0.7466681985294117, f1: 0.8099162992780014, roc_auc: 0.8240699490699491\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6527214050292969, acc :0.5714285714285714, f1: 0.6147859922178989, roc_auc: 0.6508924553772312\n",
      "bce:0.5387207865715027, acc :0.7613740808823529, f1: 0.8047554755475548, roc_auc: 0.8031334906334906\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.677701473236084, acc :0.5497835497835498, f1: 0.4851485148514852, roc_auc: 0.5503960242273644\n",
      "bce:0.5138731300830841, acc :0.7721737132352942, f1: 0.8221534227726178, roc_auc: 0.813232531982532\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6508514285087585, acc :0.6796536796536796, f1: 0.6636363636363636, roc_auc: 0.7064564564564565\n",
      "bce:0.5327818244695663, acc :0.7760799632352942, f1: 0.828130245048674, roc_auc: 0.8104392166892167\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.6056687831878662, acc :0.7155172413793104, f1: 0.8124999999999999, roc_auc: 0.6624493927125507\n",
      "bce:0.5857762098312378, acc :0.7163373161764706, f1: 0.7822835185947578, roc_auc: 0.7545443795443795\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6111372709274292, acc :0.6767241379310345, f1: 0.7800586510263929, roc_auc: 0.6579431772709085\n",
      "bce:0.483722984790802, acc :0.7466681985294117, f1: 0.8099162992780014, roc_auc: 0.8240699490699491\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6527214050292969, acc :0.5714285714285714, f1: 0.6147859922178989, roc_auc: 0.6508924553772312\n",
      "bce:0.5387208163738251, acc :0.7613740808823529, f1: 0.8047554755475548, roc_auc: 0.8031334906334906\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6777014136314392, acc :0.5497835497835498, f1: 0.4851485148514852, roc_auc: 0.5503960242273644\n",
      "bce:0.5138731300830841, acc :0.7721737132352942, f1: 0.8221534227726178, roc_auc: 0.813232531982532\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6508514285087585, acc :0.6796536796536796, f1: 0.6636363636363636, roc_auc: 0.7064564564564565\n",
      "bce:0.532781794667244, acc :0.7760799632352942, f1: 0.828130245048674, roc_auc: 0.8104392166892167\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.6056687831878662, acc :0.7155172413793104, f1: 0.8124999999999999, roc_auc: 0.6624493927125507\n",
      "bce:0.5857762098312378, acc :0.7163373161764706, f1: 0.7822835185947578, roc_auc: 0.7545443795443795\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6111372709274292, acc :0.6767241379310345, f1: 0.7800586510263929, roc_auc: 0.6579431772709085\n",
      "bce:0.4837230145931244, acc :0.7466681985294117, f1: 0.8099162992780014, roc_auc: 0.8240699490699491\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6527214050292969, acc :0.5714285714285714, f1: 0.6147859922178989, roc_auc: 0.6508924553772312\n",
      "bce:0.5387207865715027, acc :0.7613740808823529, f1: 0.8047554755475548, roc_auc: 0.8031334906334906\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.677701473236084, acc :0.5497835497835498, f1: 0.4851485148514852, roc_auc: 0.5503960242273644\n",
      "bce:0.5138731300830841, acc :0.7721737132352942, f1: 0.8221534227726178, roc_auc: 0.813232531982532\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6508514285087585, acc :0.6796536796536796, f1: 0.6636363636363636, roc_auc: 0.7064564564564565\n",
      "bce:0.5327818244695663, acc :0.7760799632352942, f1: 0.828130245048674, roc_auc: 0.8104392166892167\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.6056686639785767, acc :0.7155172413793104, f1: 0.8124999999999999, roc_auc: 0.6624493927125507\n",
      "bce:0.5857762098312378, acc :0.7163373161764706, f1: 0.7822835185947578, roc_auc: 0.7545443795443795\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6111372709274292, acc :0.6767241379310345, f1: 0.7800586510263929, roc_auc: 0.6579431772709085\n",
      "bce:0.483722984790802, acc :0.7466681985294117, f1: 0.8099162992780014, roc_auc: 0.8240699490699491\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6527214050292969, acc :0.5714285714285714, f1: 0.6147859922178989, roc_auc: 0.6508924553772312\n",
      "bce:0.5387207865715027, acc :0.7613740808823529, f1: 0.8047554755475548, roc_auc: 0.8031334906334906\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.677701473236084, acc :0.5497835497835498, f1: 0.4851485148514852, roc_auc: 0.5503960242273644\n",
      "bce:0.5138731300830841, acc :0.7721737132352942, f1: 0.8221534227726178, roc_auc: 0.813232531982532\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6508514285087585, acc :0.6796536796536796, f1: 0.6636363636363636, roc_auc: 0.7064564564564565\n",
      "bce:0.5327818244695663, acc :0.7760799632352942, f1: 0.828130245048674, roc_auc: 0.8104392166892167\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.6056686639785767, acc :0.7155172413793104, f1: 0.8124999999999999, roc_auc: 0.6624493927125507\n",
      "bce:0.5857762396335602, acc :0.7163373161764706, f1: 0.7822835185947578, roc_auc: 0.7545443795443795\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6111372709274292, acc :0.6767241379310345, f1: 0.7800586510263929, roc_auc: 0.6579431772709085\n",
      "bce:0.4837230145931244, acc :0.7466681985294117, f1: 0.8099162992780014, roc_auc: 0.8240699490699491\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6527214050292969, acc :0.5714285714285714, f1: 0.6147859922178989, roc_auc: 0.6508924553772312\n",
      "bce:0.5387207865715027, acc :0.7613740808823529, f1: 0.8047554755475548, roc_auc: 0.8031334906334906\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6777014136314392, acc :0.5497835497835498, f1: 0.4851485148514852, roc_auc: 0.5503960242273644\n",
      "bce:0.5138731300830841, acc :0.7721737132352942, f1: 0.8221534227726178, roc_auc: 0.813232531982532\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6508513689041138, acc :0.6796536796536796, f1: 0.6636363636363636, roc_auc: 0.7064564564564565\n",
      "bce:0.532781794667244, acc :0.7760799632352942, f1: 0.828130245048674, roc_auc: 0.8104392166892167\n",
      "validation bce:0.640±0.027\n",
      "validation acc:0.639±0.066\n",
      "validation f1: 0.671±0.118\n",
      "validation roc_auc: 0.646±0.051\n",
      "bce:0.531±0.033\n",
      "acc:0.755±0.022\n",
      "f1: 0.809±0.016\n",
      "roc_auc: 0.801±0.024\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'freeze'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we evaluated other methods for trf learning \n",
    "1. Using the best performance model --> pretrained at 60 epochs using high similarity data\n",
    "2. We investigated the prediction performances by using different learning rate at the feature extraction block. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning at 5x slower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5807152986526489, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7156713900134953\n",
      "bce:0.45497073233127594, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8720691845691846\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6174641251564026, acc :0.6508620689655172, f1: 0.7552870090634441, roc_auc: 0.6572228891556624\n",
      "bce:0.4905737042427063, acc :0.8025045955882353, f1: 0.841635007738849, roc_auc: 0.8588451088451088\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6593272089958191, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6376181190940453\n",
      "bce:0.4854661375284195, acc :0.7760799632352942, f1: 0.8206502322257949, roc_auc: 0.8537947287947288\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6443107724189758, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6347258891132164\n",
      "bce:0.4665004014968872, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8652890527890529\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6248038411140442, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7334084084084084\n",
      "bce:0.48666803538799286, acc :0.7936580882352942, f1: 0.8303701544739143, roc_auc: 0.8569479506979507\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.579926073551178, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7167678812415654\n",
      "bce:0.45550337433815, acc :0.7975643382352942, f1: 0.8393489030431707, roc_auc: 0.874526312026312\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6173752546310425, acc :0.6508620689655172, f1: 0.7552870090634441, roc_auc: 0.6570628251300521\n",
      "bce:0.49052639305591583, acc :0.8025045955882353, f1: 0.841635007738849, roc_auc: 0.8590327965327966\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6590568423271179, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6378431078446077\n",
      "bce:0.4852931499481201, acc :0.7760799632352942, f1: 0.8206502322257949, roc_auc: 0.8539511352011353\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6443515419960022, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6345705854946421\n",
      "bce:0.46644918620586395, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8653203340703342\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.625164270401001, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7330330330330331\n",
      "bce:0.4866827130317688, acc :0.7917049632352942, f1: 0.8290610703652603, roc_auc: 0.8567602630102631\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5811009407043457, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7167678812415655\n",
      "bce:0.45192965865135193, acc :0.7975643382352942, f1: 0.840014064697609, roc_auc: 0.8752457814957815\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6172327995300293, acc :0.6508620689655172, f1: 0.7552870090634441, roc_auc: 0.6578631452581033\n",
      "bce:0.4906574636697769, acc :0.8025045955882353, f1: 0.841635007738849, roc_auc: 0.8590640778140779\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6594941020011902, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6367181640917954\n",
      "bce:0.48521849513053894, acc :0.7760799632352942, f1: 0.8206502322257949, roc_auc: 0.8539198539198539\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6442784070968628, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6345705854946421\n",
      "bce:0.4664585292339325, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.865195208945209\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6259990334510803, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.733033033033033\n",
      "bce:0.486647292971611, acc :0.7917049632352942, f1: 0.8290610703652603, roc_auc: 0.856979231979232\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5803151726722717, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7173582995951417\n",
      "bce:0.45334239304065704, acc :0.7975643382352942, f1: 0.8393489030431707, roc_auc: 0.8711448398948399\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6173888444900513, acc :0.6508620689655172, f1: 0.7552870090634441, roc_auc: 0.6569827931172469\n",
      "bce:0.49051083624362946, acc :0.8025045955882353, f1: 0.841635007738849, roc_auc: 0.8590640778140779\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.659155011177063, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6381430928453578\n",
      "bce:0.48582130670547485, acc :0.7741268382352942, f1: 0.8194444444444444, roc_auc: 0.8535444785444786\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6443111300468445, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6347258891132164\n",
      "bce:0.46650467813014984, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8652890527890529\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6252533793449402, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7334084084084085\n",
      "bce:0.48631395399570465, acc :0.7956112132352942, f1: 0.8324740396485367, roc_auc: 0.8588107338107338\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5802843570709229, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7164304993252363\n",
      "bce:0.45340362191200256, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8741509366509366\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6173462867736816, acc :0.6508620689655172, f1: 0.7552870090634441, roc_auc: 0.657703081232493\n",
      "bce:0.4907221496105194, acc :0.8025045955882353, f1: 0.841635007738849, roc_auc: 0.8590015152515152\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6595884561538696, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6367181640917954\n",
      "bce:0.4854123294353485, acc :0.7741268382352942, f1: 0.8194444444444444, roc_auc: 0.8539198539198539\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6443515419960022, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6345705854946421\n",
      "bce:0.46644917130470276, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8653203340703342\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6257581114768982, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7331831831831832\n",
      "bce:0.486711323261261, acc :0.7936580882352942, f1: 0.8311622521348965, roc_auc: 0.8566351378851379\n",
      "validation bce:0.625±0.027\n",
      "validation acc:0.648±0.034\n",
      "validation f1: 0.673±0.095\n",
      "validation roc_auc: 0.676±0.041\n",
      "bce:0.477±0.014\n",
      "acc:0.793±0.009\n",
      "f1: 0.833±0.008\n",
      "roc_auc: 0.862±0.007\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        \n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning at 10x slower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.574695885181427, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7223346828609987\n",
      "bce:0.4711271822452545, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8682982432982433\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6116788387298584, acc :0.6982758620689655, f1: 0.7852760736196319, roc_auc: 0.6743497398959583\n",
      "bce:0.4717002958059311, acc :0.7897518382352942, f1: 0.8315412186379929, roc_auc: 0.8591376403876405\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6412062644958496, acc :0.5844155844155844, f1: 0.5752212389380531, roc_auc: 0.6643917804109795\n",
      "bce:0.45823684334754944, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8719471531971532\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6643053293228149, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.48230066895484924, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8587027962027962\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6141497492790222, acc :0.7056277056277056, f1: 0.690909090909091, roc_auc: 0.7237987987987987\n",
      "bce:0.5204329192638397, acc :0.7672334558823529, f1: 0.8139837869137232, roc_auc: 0.8248832623832624\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5754681825637817, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.7201417004048583\n",
      "bce:0.47175563871860504, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.868923868923869\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6116745471954346, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6773109243697479\n",
      "bce:0.4747907817363739, acc :0.7877987132352942, f1: 0.8302965344765989, roc_auc: 0.8651639276639276\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6410044431686401, acc :0.5844155844155844, f1: 0.5752212389380531, roc_auc: 0.6657417129143544\n",
      "bce:0.45813605189323425, acc :0.8025045955882353, f1: 0.8393600229588176, roc_auc: 0.8718533093533094\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6643589735031128, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6109644354713465\n",
      "bce:0.48241637647151947, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8587340774840775\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6141008138656616, acc :0.7056277056277056, f1: 0.690909090909091, roc_auc: 0.7239489489489489\n",
      "bce:0.5204423367977142, acc :0.7672334558823529, f1: 0.8139837869137232, roc_auc: 0.8247894185394186\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5753472447395325, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.7212381916329285\n",
      "bce:0.4703093022108078, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8686736186736186\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6109572649002075, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6782713085234093\n",
      "bce:0.4742611050605774, acc :0.7917049632352942, f1: 0.8335119685602002, roc_auc: 0.8657269907269908\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6405396461486816, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6667166641667915\n",
      "bce:0.4609966427087784, acc :0.7985983455882353, f1: 0.8376292309871124, roc_auc: 0.8672205859705859\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6643620133399963, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.48236237466335297, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8587653587653588\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6141498684883118, acc :0.7056277056277056, f1: 0.690909090909091, roc_auc: 0.7237987987987987\n",
      "bce:0.5204330235719681, acc :0.7672334558823529, f1: 0.8139837869137232, roc_auc: 0.8248832623832624\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5745231509208679, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7213225371120107\n",
      "bce:0.47039899230003357, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8688613063613064\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.611336350440979, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6777911164465786\n",
      "bce:0.4742923676967621, acc :0.7858455882352942, f1: 0.829794762915782, roc_auc: 0.8651639276639277\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6407910585403442, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6667916604169791\n",
      "bce:0.46085652709007263, acc :0.7966452205882353, f1: 0.8363918690005647, roc_auc: 0.8677523677523677\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6644200086593628, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.4822882115840912, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8586402336402337\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6141607761383057, acc :0.7056277056277056, f1: 0.690909090909091, roc_auc: 0.7239489489489489\n",
      "bce:0.520390123128891, acc :0.7672334558823529, f1: 0.8139837869137232, roc_auc: 0.8249771062271063\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5749188661575317, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7215755735492577\n",
      "bce:0.4719545394182205, acc :0.7956112132352942, f1: 0.8407292741658068, roc_auc: 0.8686736186736187\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6109603047370911, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6788315326130452\n",
      "bce:0.473981112241745, acc :0.7897518382352942, f1: 0.8322649572649572, roc_auc: 0.8656018656018656\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.640860378742218, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6663416829158542\n",
      "bce:0.46102119982242584, acc :0.7966452205882353, f1: 0.8363918690005647, roc_auc: 0.8676272426272427\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6644081473350525, acc :0.5974025974025974, f1: 0.5079365079365079, roc_auc: 0.6101879173784749\n",
      "bce:0.481694757938385, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.860972235972236\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6141611933708191, acc :0.7056277056277056, f1: 0.690909090909091, roc_auc: 0.7239489489489489\n",
      "bce:0.5203900784254074, acc :0.7672334558823529, f1: 0.8139837869137232, roc_auc: 0.8249771062271063\n",
      "validation bce:0.621±0.030\n",
      "validation acc:0.656±0.051\n",
      "validation f1: 0.672±0.109\n",
      "validation roc_auc: 0.680±0.042\n",
      "bce:0.481±0.021\n",
      "acc:0.787±0.013\n",
      "f1: 0.830±0.010\n",
      "roc_auc: 0.857±0.017\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_10x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        \n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune with normal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5660783052444458, acc :0.7112068965517241, f1: 0.809116809116809, roc_auc: 0.7068994601889339\n",
      "bce:0.4877791404724121, acc :0.7927389705882353, f1: 0.8440957036274762, roc_auc: 0.8363555863555863\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6468179821968079, acc :0.646551724137931, f1: 0.7616279069767442, roc_auc: 0.5793517406962785\n",
      "bce:0.5063085556030273, acc :0.7849264705882353, f1: 0.8291332040985876, roc_auc: 0.8213264775764776\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6615083813667297, acc :0.6060606060606061, f1: 0.6459143968871596, roc_auc: 0.6379181040947952\n",
      "bce:0.4838125556707382, acc :0.7721737132352942, f1: 0.8232418464976605, roc_auc: 0.8354999917499917\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6493971943855286, acc :0.5670995670995671, f1: 0.5327102803738318, roc_auc: 0.6433452399440907\n",
      "bce:0.5150824189186096, acc :0.7143841911764706, f1: 0.785625, roc_auc: 0.8207916020416022\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6272428631782532, acc :0.7056277056277056, f1: 0.7166666666666666, roc_auc: 0.7141141141141141\n",
      "bce:0.5726665556430817, acc :0.7457490808823529, f1: 0.7972421556212186, roc_auc: 0.7646451396451396\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5688660740852356, acc :0.7112068965517241, f1: 0.8101983002832861, roc_auc: 0.7038630229419705\n",
      "bce:0.4918023496866226, acc :0.7780330882352942, f1: 0.8343824751353393, roc_auc: 0.8332728332728332\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6379701495170593, acc :0.6422413793103449, f1: 0.7522388059701492, roc_auc: 0.65578231292517\n",
      "bce:0.46276190876960754, acc :0.7985983455882353, f1: 0.8449358449358448, roc_auc: 0.8681528369028368\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6644738912582397, acc :0.5714285714285714, f1: 0.6574394463667821, roc_auc: 0.6438428078596071\n",
      "bce:0.48394738137722015, acc :0.7673483455882353, f1: 0.8255599472990777, roc_auc: 0.8361802736802737\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6494022607803345, acc :0.5670995670995671, f1: 0.5327102803738318, roc_auc: 0.6433452399440907\n",
      "bce:0.5150880813598633, acc :0.7143841911764706, f1: 0.785625, roc_auc: 0.8207916020416022\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6272428631782532, acc :0.7056277056277056, f1: 0.7166666666666666, roc_auc: 0.7141141141141141\n",
      "bce:0.5726665556430817, acc :0.7457490808823529, f1: 0.7972421556212186, roc_auc: 0.7646451396451396\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5556640625, acc :0.7025862068965517, f1: 0.8067226890756303, roc_auc: 0.7169365721997301\n",
      "bce:0.4794021546840668, acc :0.7741268382352942, f1: 0.831727681438664, roc_auc: 0.844759251009251\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6468179821968079, acc :0.646551724137931, f1: 0.7616279069767442, roc_auc: 0.5793517406962785\n",
      "bce:0.5063086003065109, acc :0.7849264705882353, f1: 0.8291332040985876, roc_auc: 0.8213264775764776\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.660213053226471, acc :0.6363636363636364, f1: 0.6692913385826771, roc_auc: 0.6449677516124194\n",
      "bce:0.4752397835254669, acc :0.7819393382352942, f1: 0.8283635326009416, roc_auc: 0.8438239063239064\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6493973135948181, acc :0.5670995670995671, f1: 0.5327102803738318, roc_auc: 0.6433452399440907\n",
      "bce:0.5150824338197708, acc :0.7143841911764706, f1: 0.785625, roc_auc: 0.8207916020416022\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6271607279777527, acc :0.70995670995671, f1: 0.7219917012448133, roc_auc: 0.7142642642642643\n",
      "bce:0.5724994540214539, acc :0.7437959558823529, f1: 0.7960729312762973, roc_auc: 0.7636754199254199\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5677810311317444, acc :0.7025862068965517, f1: 0.8077994428969361, roc_auc: 0.7060560053981106\n",
      "bce:0.5017726123332977, acc :0.7643612132352942, f1: 0.8299071021875937, roc_auc: 0.8381386193886193\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.646818220615387, acc :0.646551724137931, f1: 0.7616279069767442, roc_auc: 0.5793517406962785\n",
      "bce:0.5063088536262512, acc :0.7849264705882353, f1: 0.8291332040985876, roc_auc: 0.8213264775764776\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6654661297798157, acc :0.5757575757575758, f1: 0.6449275362318839, roc_auc: 0.6362681865906705\n",
      "bce:0.48262743651866913, acc :0.7712545955882353, f1: 0.8254653358510924, roc_auc: 0.8389986827486827\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6493971943855286, acc :0.5670995670995671, f1: 0.5327102803738318, roc_auc: 0.6433452399440907\n",
      "bce:0.5150824189186096, acc :0.7143841911764706, f1: 0.785625, roc_auc: 0.8207916020416022\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6272428631782532, acc :0.7056277056277056, f1: 0.7166666666666666, roc_auc: 0.7141141141141141\n",
      "bce:0.572666585445404, acc :0.7457490808823529, f1: 0.7972421556212186, roc_auc: 0.7646451396451396\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5660170316696167, acc :0.7198275862068966, f1: 0.8158640226628895, roc_auc: 0.7071524966261808\n",
      "bce:0.4905799478292465, acc :0.7760799632352942, f1: 0.8339052848318462, roc_auc: 0.8307531432531432\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6455932259559631, acc :0.6551724137931034, f1: 0.7714285714285715, roc_auc: 0.5836734693877552\n",
      "bce:0.502397894859314, acc :0.7672334558823529, f1: 0.8211477987421385, roc_auc: 0.8330287705287706\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6586747169494629, acc :0.6190476190476191, f1: 0.6535433070866141, roc_auc: 0.6445177741112945\n",
      "bce:0.474356546998024, acc :0.7702205882352942, f1: 0.819909245604084, roc_auc: 0.8416654979154978\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6494022607803345, acc :0.5670995670995671, f1: 0.5327102803738318, roc_auc: 0.6433452399440907\n",
      "bce:0.5150881111621857, acc :0.7143841911764706, f1: 0.785625, roc_auc: 0.8207916020416022\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6272428631782532, acc :0.7056277056277056, f1: 0.7166666666666666, roc_auc: 0.7141141141141141\n",
      "bce:0.5726665556430817, acc :0.7457490808823529, f1: 0.7972421556212186, roc_auc: 0.7646451396451396\n",
      "validation bce:0.630±0.034\n",
      "validation acc:0.646±0.058\n",
      "validation f1: 0.695±0.096\n",
      "validation roc_auc: 0.661±0.047\n",
      "bce:0.511±0.034\n",
      "acc:0.759±0.027\n",
      "f1: 0.815±0.020\n",
      "roc_auc: 0.819±0.029\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune'\n",
    "params = best_params_vertical\n",
    "es_trigger = 0\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the use of slower learning rate + force training the model \n",
    "1. Force training at 10, 15 and 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5815671682357788, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7169365721997301\n",
      "bce:0.45119190216064453, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8759026884026884\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6222043633460999, acc :0.6508620689655172, f1: 0.7492260061919505, roc_auc: 0.6472989195678271\n",
      "bce:0.475962296128273, acc :0.7995174632352942, f1: 0.835016835016835, roc_auc: 0.8651013651013651\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.659091055393219, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.637543122843858\n",
      "bce:0.48530712723731995, acc :0.7780330882352942, f1: 0.8218637992831541, roc_auc: 0.8538885726385727\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6443515419960022, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6345705854946421\n",
      "bce:0.46644918620586395, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8653203340703342\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6256555914878845, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7331081081081081\n",
      "bce:0.4864102751016617, acc :0.7956112132352942, f1: 0.8324740396485367, roc_auc: 0.8573233260733262\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5802918672561646, acc :0.6681034482758621, f1: 0.7659574468085107, roc_auc: 0.7167678812415655\n",
      "bce:0.4528038054704666, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8744011869011868\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6224626898765564, acc :0.646551724137931, f1: 0.7453416149068323, roc_auc: 0.647218887555022\n",
      "bce:0.4755740612745285, acc :0.7975643382352942, f1: 0.832957957957958, roc_auc: 0.8657895532895533\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6590244770050049, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6379931003449828\n",
      "bce:0.48547273874282837, acc :0.7780330882352942, f1: 0.8218637992831541, roc_auc: 0.8537634475134475\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6443521976470947, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6345705854946421\n",
      "bce:0.46644817292690277, acc :0.7936580882352942, f1: 0.8303504847129008, roc_auc: 0.8653516153516154\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6257787942886353, acc :0.683982683982684, f1: 0.7020408163265306, roc_auc: 0.7297297297297297\n",
      "bce:0.49072349071502686, acc :0.7897518382352942, f1: 0.8324294256709776, roc_auc: 0.852238570988571\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.580640971660614, acc :0.6637931034482759, f1: 0.7636363636363636, roc_auc: 0.7170209176788124\n",
      "bce:0.45222483575344086, acc :0.7975643382352942, f1: 0.840014064697609, roc_auc: 0.874870406120406\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6218795776367188, acc :0.6508620689655172, f1: 0.7492260061919505, roc_auc: 0.6486594637855142\n",
      "bce:0.4758552014827728, acc :0.7975643382352942, f1: 0.832957957957958, roc_auc: 0.8657582720082719\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.659309446811676, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6376931153442328\n",
      "bce:0.48523111641407013, acc :0.7741268382352942, f1: 0.8194444444444444, roc_auc: 0.8540762603262604\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6441127061843872, acc :0.6363636363636364, f1: 0.5172413793103448, roc_auc: 0.6352694517782265\n",
      "bce:0.4668981432914734, acc :0.7917049632352942, f1: 0.8282828282828283, roc_auc: 0.8653828966328966\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.625257670879364, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7334084084084085\n",
      "bce:0.4863109886646271, acc :0.7956112132352942, f1: 0.8324740396485367, roc_auc: 0.8588107338107338\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5800713896751404, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7165991902834008\n",
      "bce:0.45346760749816895, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8742134992134991\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6218757629394531, acc :0.6508620689655172, f1: 0.7492260061919505, roc_auc: 0.6488995598239296\n",
      "bce:0.47591955959796906, acc :0.7995174632352942, f1: 0.8342749529190208, roc_auc: 0.8653203340703342\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6586989760398865, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6385180740962952\n",
      "bce:0.4851267337799072, acc :0.7780330882352942, f1: 0.8218637992831541, roc_auc: 0.8540136977636977\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6445715427398682, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6347258891132163\n",
      "bce:0.4663035720586777, acc :0.7956112132352942, f1: 0.8324043106651804, roc_auc: 0.8655080217580218\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.625709593296051, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7334084084084084\n",
      "bce:0.48649923503398895, acc :0.7936580882352942, f1: 0.8303701544739143, roc_auc: 0.8568541068541069\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5811117887496948, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7133097165991903\n",
      "bce:0.4536949396133423, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8728199353199353\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6220053434371948, acc :0.646551724137931, f1: 0.7453416149068323, roc_auc: 0.6489795918367346\n",
      "bce:0.47584399580955505, acc :0.7995174632352942, f1: 0.8342749529190208, roc_auc: 0.8658521158521159\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6593354344367981, acc :0.5930735930735931, f1: 0.6209677419354839, roc_auc: 0.6369431528423579\n",
      "bce:0.4855100065469742, acc :0.7780330882352942, f1: 0.8218637992831541, roc_auc: 0.8536383223883224\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6448415517807007, acc :0.6320346320346321, f1: 0.5142857142857143, roc_auc: 0.6346482373039292\n",
      "bce:0.4665016680955887, acc :0.7956112132352942, f1: 0.8324043106651804, roc_auc: 0.8654454591954592\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6254689693450928, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7334834834834836\n",
      "bce:0.4865974336862564, acc :0.7936580882352942, f1: 0.8303701544739143, roc_auc: 0.8571356383856384\n",
      "validation bce:0.626±0.026\n",
      "validation acc:0.647±0.033\n",
      "validation f1: 0.671±0.093\n",
      "validation roc_auc: 0.674±0.042\n",
      "bce:0.474±0.013\n",
      "acc:0.793±0.008\n",
      "f1: 0.832±0.006\n",
      "roc_auc: 0.863±0.007\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 10\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "\n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.580559253692627, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7139844804318489\n",
      "bce:0.4556100368499756, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8675131175131175\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6257521510124207, acc :0.6810344827586207, f1: 0.7672955974842767, roc_auc: 0.6517006802721088\n",
      "bce:0.4457445442676544, acc :0.8083639705882353, f1: 0.8475473801560758, roc_auc: 0.8695261195261195\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6616900563240051, acc :0.5670995670995671, f1: 0.5614035087719298, roc_auc: 0.64076796160192\n",
      "bce:0.4612022638320923, acc :0.8014705882352942, f1: 0.8425336164189667, roc_auc: 0.8504352566852567\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6624472737312317, acc :0.5757575757575758, f1: 0.4302325581395348, roc_auc: 0.6218356887715484\n",
      "bce:0.4452606290578842, acc :0.8200827205882353, f1: 0.8505186089078705, roc_auc: 0.8666795229295229\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.649895191192627, acc :0.7012987012987013, f1: 0.7112970711297071, roc_auc: 0.7379879879879879\n",
      "bce:0.4637737274169922, acc :0.7848115808823529, f1: 0.8250432525951558, roc_auc: 0.8683061495561495\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5802731513977051, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7166835357624831\n",
      "bce:0.45391352474689484, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8746827184327184\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6258406639099121, acc :0.6767241379310345, f1: 0.7648902821316615, roc_auc: 0.6520208083233293\n",
      "bce:0.4451967626810074, acc :0.8083639705882353, f1: 0.8475473801560758, roc_auc: 0.8673192423192423\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6618324518203735, acc :0.5714285714285714, f1: 0.5638766519823789, roc_auc: 0.6417429128543574\n",
      "bce:0.4611981064081192, acc :0.8014705882352942, f1: 0.8425336164189667, roc_auc: 0.8486350361350361\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6628599166870117, acc :0.5757575757575758, f1: 0.4302325581395348, roc_auc: 0.6205932598229538\n",
      "bce:0.4450652599334717, acc :0.8220358455882353, f1: 0.8525387655822437, roc_auc: 0.8671800234300235\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6522197723388672, acc :0.696969696969697, f1: 0.7083333333333331, roc_auc: 0.7374624624624624\n",
      "bce:0.46449223160743713, acc :0.7867647058823529, f1: 0.8271551724137931, roc_auc: 0.8679933367433368\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5776359438896179, acc :0.6810344827586207, f1: 0.7784431137724552, roc_auc: 0.7180330634278003\n",
      "bce:0.45554421842098236, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8697199947199947\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6260856986045837, acc :0.6637931034482759, f1: 0.7577639751552796, roc_auc: 0.6514605842336935\n",
      "bce:0.44498826563358307, acc :0.8083639705882353, f1: 0.8482414843533648, roc_auc: 0.8675382112882113\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6617580652236938, acc :0.5670995670995671, f1: 0.5614035087719298, roc_auc: 0.6418929053547323\n",
      "bce:0.462454617023468, acc :0.7995174632352942, f1: 0.840610578629748, roc_auc: 0.8479781292281292\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6631038188934326, acc :0.5757575757575758, f1: 0.4302325581395348, roc_auc: 0.6202826525858053\n",
      "bce:0.44566041231155396, acc :0.8161764705882353, f1: 0.8478787878787878, roc_auc: 0.8686987124487124\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6530859470367432, acc :0.696969696969697, f1: 0.7083333333333331, roc_auc: 0.7373123123123123\n",
      "bce:0.4649272561073303, acc :0.7848115808823529, f1: 0.8250432525951558, roc_auc: 0.86880665005665\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5793794393539429, acc :0.6681034482758621, f1: 0.7673716012084593, roc_auc: 0.7173582995951417\n",
      "bce:0.45333127677440643, acc :0.7975643382352942, f1: 0.8393489030431707, roc_auc: 0.8754960317460317\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6264139413833618, acc :0.6724137931034483, f1: 0.7625, roc_auc: 0.6510604241696678\n",
      "bce:0.4456564337015152, acc :0.8103170955882353, f1: 0.8494929851368246, roc_auc: 0.8674130861630861\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6620385646820068, acc :0.5670995670995671, f1: 0.5575221238938053, roc_auc: 0.6400929953502326\n",
      "bce:0.46191635727882385, acc :0.7975643382352942, f1: 0.8393489030431707, roc_auc: 0.8482283794783795\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6625810265541077, acc :0.5757575757575758, f1: 0.4302325581395348, roc_auc: 0.6214474297251126\n",
      "bce:0.44511716067790985, acc :0.8200827205882353, f1: 0.8505186089078705, roc_auc: 0.8667733667733668\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6513567566871643, acc :0.696969696969697, f1: 0.7058823529411766, roc_auc: 0.7382882882882882\n",
      "bce:0.4643258899450302, acc :0.7848115808823529, f1: 0.8250432525951558, roc_auc: 0.8689317751817753\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5807581543922424, acc :0.6637931034482759, f1: 0.7636363636363636, roc_auc: 0.7170209176788125\n",
      "bce:0.453675240278244, acc :0.7995174632352942, f1: 0.8412698412698413, roc_auc: 0.8751206563706564\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.626907467842102, acc :0.6767241379310345, f1: 0.7648902821316615, roc_auc: 0.6509803921568627\n",
      "bce:0.44453316926956177, acc :0.8083639705882353, f1: 0.8475473801560758, roc_auc: 0.8655190217690217\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6622728705406189, acc :0.5584415584415584, f1: 0.5486725663716814, roc_auc: 0.6408429578521073\n",
      "bce:0.4621032476425171, acc :0.8034237132352942, f1: 0.8438054668086618, roc_auc: 0.848071973071973\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6623440980911255, acc :0.5714285714285714, f1: 0.4210526315789473, roc_auc: 0.6203603043950924\n",
      "bce:0.44530028104782104, acc :0.8220358455882353, f1: 0.8525387655822437, roc_auc: 0.8668359293359293\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.651580810546875, acc :0.696969696969697, f1: 0.7083333333333331, roc_auc: 0.7379129129129127\n",
      "bce:0.4643952399492264, acc :0.7848115808823529, f1: 0.8250432525951558, roc_auc: 0.8680558993058993\n",
      "validation bce:0.636±0.031\n",
      "validation acc:0.637±0.055\n",
      "validation f1: 0.646±0.133\n",
      "validation roc_auc: 0.674±0.045\n",
      "bce:0.454±0.008\n",
      "acc:0.803±0.012\n",
      "f1: 0.841±0.009\n",
      "roc_auc: 0.865±0.008\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 15\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5835076570510864, acc :0.6896551724137931, f1: 0.7791411042944786, roc_auc: 0.710357624831309\n",
      "bce:0.43635958433151245, acc :0.8103170955882353, f1: 0.8459606830248243, roc_auc: 0.8820338195338195\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6529975533485413, acc :0.6336206896551724, f1: 0.7301587301587302, roc_auc: 0.6368947579031614\n",
      "bce:0.43248628079891205, acc :0.8269761029411764, f1: 0.8602150537634409, roc_auc: 0.8587763587763588\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6707444190979004, acc :0.5800865800865801, f1: 0.5610859728506787, roc_auc: 0.6424178791060448\n",
      "bce:0.4682886451482773, acc :0.7995174632352942, f1: 0.8359972729338687, roc_auc: 0.8396339333839333\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6685404777526855, acc :0.5887445887445888, f1: 0.45714285714285713, roc_auc: 0.6347258891132164\n",
      "bce:0.4401945322751999, acc :0.8220358455882353, f1: 0.8525387655822437, roc_auc: 0.8699186824186824\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6743557453155518, acc :0.6883116883116883, f1: 0.7024793388429752, roc_auc: 0.7391141141141141\n",
      "bce:0.46946485340595245, acc :0.7945772058823529, f1: 0.8388157894736842, roc_auc: 0.8685986810986812\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5858575701713562, acc :0.6896551724137931, f1: 0.7791411042944786, roc_auc: 0.7115384615384616\n",
      "bce:0.4353417903184891, acc :0.7975643382352942, f1: 0.8351729212656365, roc_auc: 0.8833022583022583\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6512733697891235, acc :0.6422413793103449, f1: 0.7381703470031545, roc_auc: 0.6398559423769508\n",
      "bce:0.4344632774591446, acc :0.8328354779411764, f1: 0.8653804930332263, roc_auc: 0.8599196724196724\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6727077960968018, acc :0.5627705627705628, f1: 0.547085201793722, roc_auc: 0.6406179691015449\n",
      "bce:0.4692750871181488, acc :0.7995174632352942, f1: 0.8353030880670989, roc_auc: 0.8382716507716508\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6697713732719421, acc :0.5800865800865801, f1: 0.4393063583815029, roc_auc: 0.6337164155924834\n",
      "bce:0.4396815001964569, acc :0.8200827205882353, f1: 0.8505186089078705, roc_auc: 0.8692617755117755\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.675248920917511, acc :0.6883116883116883, f1: 0.7024793388429752, roc_auc: 0.7391891891891892\n",
      "bce:0.46992792189121246, acc :0.7926240808823529, f1: 0.8375, roc_auc: 0.8702737765237766\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5854445695877075, acc :0.6896551724137931, f1: 0.7844311377245509, roc_auc: 0.7024291497975708\n",
      "bce:0.4602836072444916, acc :0.7760799632352942, f1: 0.8315412186379929, roc_auc: 0.8622688622688623\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6514326930046082, acc :0.6422413793103449, f1: 0.7381703470031545, roc_auc: 0.6395358143257303\n",
      "bce:0.434542179107666, acc :0.8308823529411764, f1: 0.8634408602150538, roc_auc: 0.8601699226699226\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6725178360939026, acc :0.5627705627705628, f1: 0.5429864253393665, roc_auc: 0.6391930403479826\n",
      "bce:0.4677559733390808, acc :0.8053768382352942, f1: 0.8405532755737954, roc_auc: 0.8407600595100595\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6685402393341064, acc :0.5844155844155844, f1: 0.4545454545454546, roc_auc: 0.6347258891132164\n",
      "bce:0.44050396978855133, acc :0.8220358455882353, f1: 0.8525387655822437, roc_auc: 0.8696371508871509\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6745046377182007, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7392642642642643\n",
      "bce:0.46925804018974304, acc :0.7945772058823529, f1: 0.8388157894736842, roc_auc: 0.8700235262735263\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5832385420799255, acc :0.6853448275862069, f1: 0.7767584097859328, roc_auc: 0.7100202429149797\n",
      "bce:0.4357738792896271, acc :0.8122702205882353, f1: 0.8479691075514875, roc_auc: 0.8818774131274131\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6514213681221008, acc :0.6508620689655172, f1: 0.7428571428571428, roc_auc: 0.6405762304921969\n",
      "bce:0.4332904815673828, acc :0.8308823529411764, f1: 0.8627705627705629, roc_auc: 0.8602324852324852\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6712846159934998, acc :0.5714285714285714, f1: 0.5520361990950227, roc_auc: 0.6407679616019198\n",
      "bce:0.46915410459041595, acc :0.7975643382352942, f1: 0.8325889419374711, roc_auc: 0.839303933053933\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6697942614555359, acc :0.5844155844155844, f1: 0.4418604651162791, roc_auc: 0.6334058083553347\n",
      "bce:0.4402066022157669, acc :0.8200827205882353, f1: 0.8505186089078705, roc_auc: 0.8693243380743381\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6722961664199829, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7406906906906907\n",
      "bce:0.4668622612953186, acc :0.7906709558823529, f1: 0.8348509933774835, roc_auc: 0.868692524942525\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5841640830039978, acc :0.6853448275862069, f1: 0.7767584097859328, roc_auc: 0.7093454790823212\n",
      "bce:0.43658603727817535, acc :0.8122702205882353, f1: 0.8479691075514875, roc_auc: 0.8816271628771628\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6520659327507019, acc :0.6422413793103449, f1: 0.7365079365079366, roc_auc: 0.6382553021208482\n",
      "bce:0.43246351182460785, acc :0.8308823529411764, f1: 0.8634408602150538, roc_auc: 0.8625957688457688\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6706883311271667, acc :0.5714285714285714, f1: 0.5560538116591929, roc_auc: 0.6406929653517324\n",
      "bce:0.4673946499824524, acc :0.7995174632352942, f1: 0.8353030880670989, roc_auc: 0.8368468055968056\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6688873767852783, acc :0.5930735930735931, f1: 0.4597701149425288, roc_auc: 0.6348035409225035\n",
      "bce:0.4401167631149292, acc :0.8200827205882353, f1: 0.8512121212121212, roc_auc: 0.8700750888250888\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6745872497558594, acc :0.6926406926406926, f1: 0.7053941908713692, roc_auc: 0.7394894894894896\n",
      "bce:0.47029270231723785, acc :0.7945772058823529, f1: 0.8388157894736842, roc_auc: 0.8695855883355883\n",
      "validation bce:0.650±0.034\n",
      "validation acc:0.635±0.050\n",
      "validation f1: 0.645±0.124\n",
      "validation roc_auc: 0.672±0.043\n",
      "bce:0.450±0.016\n",
      "acc:0.809±0.015\n",
      "f1: 0.846±0.011\n",
      "roc_auc: 0.863±0.014\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 20\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "Epoch: 1/300, train loss : 1.3573159873485565, validation loss : 0.6672883033752441\n",
      "Epoch: 2/300, train loss : 0.7439092099666595, validation loss : 0.6624502539634705\n",
      "Epoch: 3/300, train loss : 0.7037086635828018, validation loss : 0.7322502732276917\n",
      "Epoch: 4/300, train loss : 0.6863243877887726, validation loss : 0.6494139432907104\n",
      "Epoch: 5/300, train loss : 0.662983849644661, validation loss : 0.6637433767318726\n",
      "Epoch: 6/300, train loss : 0.6509369611740112, validation loss : 0.6498183012008667\n",
      "Epoch: 7/300, train loss : 0.6347012966871262, validation loss : 0.6043581366539001\n",
      "Epoch: 8/300, train loss : 0.6238026469945908, validation loss : 0.6012547612190247\n",
      "Epoch: 9/300, train loss : 0.6111105382442474, validation loss : 0.6027132272720337\n",
      "Epoch: 10/300, train loss : 0.6114585101604462, validation loss : 0.5991734266281128\n",
      "Epoch: 11/300, train loss : 0.5966641157865524, validation loss : 0.606228768825531\n",
      "Epoch: 12/300, train loss : 0.5986240655183792, validation loss : 0.6034618616104126\n",
      "Epoch: 13/300, train loss : 0.5871856957674026, validation loss : 0.5993529558181763\n",
      "Epoch: 14/300, train loss : 0.5852290093898773, validation loss : 0.5938898324966431\n",
      "Epoch: 15/300, train loss : 0.5830007046461105, validation loss : 0.5933277606964111\n",
      "Epoch: 16/300, train loss : 0.5744713693857193, validation loss : 0.6020744442939758\n",
      "Epoch: 17/300, train loss : 0.5627912431955338, validation loss : 0.5908869504928589\n",
      "Epoch: 18/300, train loss : 0.554110199213028, validation loss : 0.6040225028991699\n",
      "Epoch: 19/300, train loss : 0.5444090813398361, validation loss : 0.5818063020706177\n",
      "Epoch: 20/300, train loss : 0.5360003337264061, validation loss : 0.5962564945220947\n",
      "Epoch: 21/300, train loss : 0.5401532053947449, validation loss : 0.5881955623626709\n",
      "Epoch: 22/300, train loss : 0.5513573586940765, validation loss : 0.589288055896759\n",
      "Epoch: 23/300, train loss : 0.5436430275440216, validation loss : 0.6374532580375671\n",
      "Epoch: 24/300, train loss : 0.5257743895053864, validation loss : 0.5864691734313965\n",
      "Epoch: 25/300, train loss : 0.5166772156953812, validation loss : 0.6358795166015625\n",
      "Epoch: 26/300, train loss : 0.503825843334198, validation loss : 0.5933919548988342\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.5022417232394218, validation loss : 0.592711329460144\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.4993830770254135, validation loss : 0.6583696007728577\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.4950566068291664, validation loss : 0.6060331463813782\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4764132648706436, validation loss : 0.6379702687263489\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.4729236885905266, validation loss : 0.6305743455886841\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.46924929320812225, validation loss : 0.6386735439300537\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.46027447283267975, validation loss : 0.6330713629722595\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.45350347459316254, validation loss : 0.6636016964912415\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4571714401245117, validation loss : 0.6571788787841797\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.4408157393336296, validation loss : 0.6509537696838379\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.42294105142354965, validation loss : 0.6534647345542908\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.4255320876836777, validation loss : 0.6625207662582397\n",
      "Early stopping...\n",
      "bce:0.592711329460144, acc :0.6939655172413793, f1: 0.7828746177370031, roc_auc: 0.7047908232118759\n",
      "bce:0.42986615002155304, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8757916570416571\n",
      "Rep no 0, Fold no 1\n",
      "Epoch: 1/300, train loss : 1.276012808084488, validation loss : 0.678240954875946\n",
      "Epoch: 2/300, train loss : 0.7270723581314087, validation loss : 0.6882150769233704\n",
      "Epoch: 3/300, train loss : 0.6953273564577103, validation loss : 0.7463355660438538\n",
      "Epoch: 4/300, train loss : 0.6708378940820694, validation loss : 0.6577580571174622\n",
      "Epoch: 5/300, train loss : 0.6570691764354706, validation loss : 0.6461412906646729\n",
      "Epoch: 6/300, train loss : 0.6288012564182281, validation loss : 0.6492577791213989\n",
      "Epoch: 7/300, train loss : 0.6158763021230698, validation loss : 0.6288970112800598\n",
      "Epoch: 8/300, train loss : 0.6109586805105209, validation loss : 0.6173445582389832\n",
      "Epoch: 9/300, train loss : 0.5968881696462631, validation loss : 0.6246638894081116\n",
      "Epoch: 10/300, train loss : 0.5931424051523209, validation loss : 0.6286402940750122\n",
      "Epoch: 11/300, train loss : 0.5797557532787323, validation loss : 0.6218794584274292\n",
      "Epoch: 12/300, train loss : 0.57607302069664, validation loss : 0.6222306489944458\n",
      "Epoch: 13/300, train loss : 0.5668315887451172, validation loss : 0.6273204684257507\n",
      "Epoch: 14/300, train loss : 0.5626460909843445, validation loss : 0.6226183176040649\n",
      "Epoch: 15/300, train loss : 0.5567274689674377, validation loss : 0.6282271146774292\n",
      "Epoch: 16/300, train loss : 0.5515022873878479, validation loss : 0.635883092880249\n",
      "Epoch: 17/300, train loss : 0.5386694669723511, validation loss : 0.6260730028152466\n",
      "Epoch: 18/300, train loss : 0.5342254936695099, validation loss : 0.6366216540336609\n",
      "Epoch: 19/300, train loss : 0.519204206764698, validation loss : 0.6360791325569153\n",
      "Epoch: 20/300, train loss : 0.5065143927931786, validation loss : 0.658933699131012\n",
      "Epoch: 21/300, train loss : 0.5167011916637421, validation loss : 0.651035726070404\n",
      "Epoch: 22/300, train loss : 0.5132972523570061, validation loss : 0.6550527215003967\n",
      "Epoch: 23/300, train loss : 0.5111109018325806, validation loss : 0.6581802368164062\n",
      "Epoch: 24/300, train loss : 0.49529967457056046, validation loss : 0.6572543382644653\n",
      "Epoch: 25/300, train loss : 0.47988297045230865, validation loss : 0.6944624185562134\n",
      "Epoch: 26/300, train loss : 0.4754605293273926, validation loss : 0.6721919178962708\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4638817235827446, validation loss : 0.692562997341156\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.45796138048171997, validation loss : 0.6846763491630554\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.4504420906305313, validation loss : 0.6974979043006897\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.4360630661249161, validation loss : 0.711514949798584\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.43399110436439514, validation loss : 0.7055783867835999\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.425761878490448, validation loss : 0.7199754118919373\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.4237019345164299, validation loss : 0.7273837327957153\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.40409648418426514, validation loss : 0.7216835021972656\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.4014440178871155, validation loss : 0.7563592195510864\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.39873287081718445, validation loss : 0.7689877152442932\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.37326177954673767, validation loss : 0.7631932497024536\n",
      "Early stopping...\n",
      "bce:0.672191858291626, acc :0.6422413793103449, f1: 0.7365079365079366, roc_auc: 0.6252100840336134\n",
      "bce:0.44597259163856506, acc :0.8308823529411764, f1: 0.8647558386411889, roc_auc: 0.8481094418594418\n",
      "Rep no 0, Fold no 2\n",
      "Epoch: 1/300, train loss : 1.3414556831121445, validation loss : 0.761675238609314\n",
      "Epoch: 2/300, train loss : 0.7178567051887512, validation loss : 0.741644024848938\n",
      "Epoch: 3/300, train loss : 0.6655277758836746, validation loss : 0.6929378509521484\n",
      "Epoch: 4/300, train loss : 0.6612867563962936, validation loss : 0.6823594570159912\n",
      "Epoch: 5/300, train loss : 0.645564004778862, validation loss : 0.6942542791366577\n",
      "Epoch: 6/300, train loss : 0.6190011352300644, validation loss : 0.6628453731536865\n",
      "Epoch: 7/300, train loss : 0.6031835824251175, validation loss : 0.6636207699775696\n",
      "Epoch: 8/300, train loss : 0.5998127460479736, validation loss : 0.6699967980384827\n",
      "Epoch: 9/300, train loss : 0.5871175080537796, validation loss : 0.6654813885688782\n",
      "Epoch: 10/300, train loss : 0.5826143175363541, validation loss : 0.6613155007362366\n",
      "Epoch: 11/300, train loss : 0.5749038904905319, validation loss : 0.6593092679977417\n",
      "Epoch: 12/300, train loss : 0.5621419250965118, validation loss : 0.6634101867675781\n",
      "Epoch: 13/300, train loss : 0.5598953366279602, validation loss : 0.6737866401672363\n",
      "Epoch: 14/300, train loss : 0.5504380911588669, validation loss : 0.6764724254608154\n",
      "Epoch: 15/300, train loss : 0.5494840741157532, validation loss : 0.6691930890083313\n",
      "Epoch: 16/300, train loss : 0.5394269973039627, validation loss : 0.6679075956344604\n",
      "Epoch: 17/300, train loss : 0.5379872769117355, validation loss : 0.6723014712333679\n",
      "Epoch: 18/300, train loss : 0.5261918157339096, validation loss : 0.6614206433296204\n",
      "Epoch: 19/300, train loss : 0.5220166891813278, validation loss : 0.661824107170105\n",
      "Epoch: 20/300, train loss : 0.5086853206157684, validation loss : 0.6728214621543884\n",
      "Epoch: 21/300, train loss : 0.5172234922647476, validation loss : 0.6838065981864929\n",
      "Epoch: 22/300, train loss : 0.505874827504158, validation loss : 0.6823136210441589\n",
      "Epoch: 23/300, train loss : 0.497672401368618, validation loss : 0.6778483986854553\n",
      "Epoch: 24/300, train loss : 0.48827601224184036, validation loss : 0.6715429425239563\n",
      "Epoch: 25/300, train loss : 0.4763222262263298, validation loss : 0.678923487663269\n",
      "Epoch: 26/300, train loss : 0.47126058489084244, validation loss : 0.7002488970756531\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46503885090351105, validation loss : 0.6910432577133179\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.4493853449821472, validation loss : 0.6904573440551758\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.4444354921579361, validation loss : 0.6969829201698303\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4410640746355057, validation loss : 0.7169041633605957\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.4211161136627197, validation loss : 0.7104580402374268\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.42123595625162125, validation loss : 0.7427123785018921\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.40783584117889404, validation loss : 0.7248926162719727\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.3990677148103714, validation loss : 0.7471494674682617\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.3989567682147026, validation loss : 0.742828905582428\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.3723509833216667, validation loss : 0.7822962999343872\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.36933258175849915, validation loss : 0.7736150026321411\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.39985013753175735, validation loss : 0.8023615479469299\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.3645022287964821, validation loss : 0.7860230803489685\n",
      "Early stopping...\n",
      "bce:0.6904574036598206, acc :0.5974025974025974, f1: 0.5903083700440529, roc_auc: 0.6436928153592321\n",
      "bce:0.4818616062402725, acc :0.7936580882352942, f1: 0.8381944444444445, roc_auc: 0.8367873367873369\n",
      "Rep no 0, Fold no 3\n",
      "Epoch: 1/300, train loss : 1.319130778312683, validation loss : 0.823249101638794\n",
      "Epoch: 2/300, train loss : 0.735413208603859, validation loss : 0.8153864741325378\n",
      "Epoch: 3/300, train loss : 0.6752772778272629, validation loss : 0.6707612872123718\n",
      "Epoch: 4/300, train loss : 0.6795459687709808, validation loss : 0.6689958572387695\n",
      "Epoch: 5/300, train loss : 0.6380354166030884, validation loss : 0.7236869931221008\n",
      "Epoch: 6/300, train loss : 0.6218600869178772, validation loss : 0.6928249597549438\n",
      "Epoch: 7/300, train loss : 0.603745311498642, validation loss : 0.6510026454925537\n",
      "Epoch: 8/300, train loss : 0.5947704911231995, validation loss : 0.6561669111251831\n",
      "Epoch: 9/300, train loss : 0.5865524709224701, validation loss : 0.676849365234375\n",
      "Epoch: 10/300, train loss : 0.5790764093399048, validation loss : 0.6802780628204346\n",
      "Epoch: 11/300, train loss : 0.5753701776266098, validation loss : 0.6511371731758118\n",
      "Epoch: 12/300, train loss : 0.5615177750587463, validation loss : 0.6442782282829285\n",
      "Epoch: 13/300, train loss : 0.5540513247251511, validation loss : 0.6601220369338989\n",
      "Epoch: 14/300, train loss : 0.5502621084451675, validation loss : 0.6647179126739502\n",
      "Epoch: 15/300, train loss : 0.5374341234564781, validation loss : 0.6493028402328491\n",
      "Epoch: 16/300, train loss : 0.5417650043964386, validation loss : 0.6700306534767151\n",
      "Epoch: 17/300, train loss : 0.52469103038311, validation loss : 0.6741932034492493\n",
      "Epoch: 18/300, train loss : 0.518232636153698, validation loss : 0.6629992127418518\n",
      "Epoch: 19/300, train loss : 0.5108993649482727, validation loss : 0.6727059483528137\n",
      "Epoch: 20/300, train loss : 0.5023933723568916, validation loss : 0.6752825975418091\n",
      "Epoch: 21/300, train loss : 0.5087642967700958, validation loss : 0.6688141226768494\n",
      "Epoch: 22/300, train loss : 0.5018187686800957, validation loss : 0.6785740852355957\n",
      "Epoch: 23/300, train loss : 0.48473168909549713, validation loss : 0.6951606273651123\n",
      "Epoch: 24/300, train loss : 0.4843137785792351, validation loss : 0.6825338006019592\n",
      "Epoch: 25/300, train loss : 0.47451046109199524, validation loss : 0.6880559325218201\n",
      "Epoch: 26/300, train loss : 0.4662272781133652, validation loss : 0.7132085561752319\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4590429291129112, validation loss : 0.6982964873313904\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.4496620446443558, validation loss : 0.7159897685050964\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.43402592092752457, validation loss : 0.7197612524032593\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.42424535006284714, validation loss : 0.7302001714706421\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.42038752138614655, validation loss : 0.7228555679321289\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.41886892914772034, validation loss : 0.7269140481948853\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.4030020013451576, validation loss : 0.7786700129508972\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4153793528676033, validation loss : 0.7571301460266113\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4064031168818474, validation loss : 0.7800158858299255\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.3812340870499611, validation loss : 0.7578061819076538\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.3722028210759163, validation loss : 0.8030256628990173\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.3750757649540901, validation loss : 0.7655819058418274\n",
      "Early stopping...\n",
      "bce:0.6982964873313904, acc :0.6060606060606061, f1: 0.48, roc_auc: 0.6408603820469017\n",
      "bce:0.4272538423538208, acc :0.8064108455882353, f1: 0.8383838383838383, roc_auc: 0.8680277117777118\n",
      "Rep no 0, Fold no 4\n",
      "Epoch: 1/300, train loss : 1.4181728810071945, validation loss : 0.7921507954597473\n",
      "Epoch: 2/300, train loss : 0.7452310472726822, validation loss : 0.7313492894172668\n",
      "Epoch: 3/300, train loss : 0.6881860792636871, validation loss : 0.6833483576774597\n",
      "Epoch: 4/300, train loss : 0.6831756979227066, validation loss : 0.6777384877204895\n",
      "Epoch: 5/300, train loss : 0.6706948578357697, validation loss : 0.6854179501533508\n",
      "Epoch: 6/300, train loss : 0.6394778192043304, validation loss : 0.6372374892234802\n",
      "Epoch: 7/300, train loss : 0.6235816031694412, validation loss : 0.6322792172431946\n",
      "Epoch: 8/300, train loss : 0.6165456920862198, validation loss : 0.6307622790336609\n",
      "Epoch: 9/300, train loss : 0.6128120720386505, validation loss : 0.6272522807121277\n",
      "Epoch: 10/300, train loss : 0.6024091094732285, validation loss : 0.62835294008255\n",
      "Epoch: 11/300, train loss : 0.5982229113578796, validation loss : 0.6258112788200378\n",
      "Epoch: 12/300, train loss : 0.5868926644325256, validation loss : 0.6257859468460083\n",
      "Epoch: 13/300, train loss : 0.5752890855073929, validation loss : 0.6342312097549438\n",
      "Epoch: 14/300, train loss : 0.5798255801200867, validation loss : 0.6374181509017944\n",
      "Epoch: 15/300, train loss : 0.5687319785356522, validation loss : 0.6389856338500977\n",
      "Epoch: 16/300, train loss : 0.5634120851755142, validation loss : 0.6613325476646423\n",
      "Epoch: 17/300, train loss : 0.5548606216907501, validation loss : 0.6521635055541992\n",
      "Epoch: 18/300, train loss : 0.5506134927272797, validation loss : 0.6579388976097107\n",
      "Epoch: 19/300, train loss : 0.5425542891025543, validation loss : 0.6653515696525574\n",
      "Epoch: 20/300, train loss : 0.5325856059789658, validation loss : 0.6675386428833008\n",
      "Epoch: 21/300, train loss : 0.5313363522291183, validation loss : 0.675601601600647\n",
      "Epoch: 22/300, train loss : 0.5287813246250153, validation loss : 0.6864162683486938\n",
      "Epoch: 23/300, train loss : 0.5056360363960266, validation loss : 0.6949962377548218\n",
      "Epoch: 24/300, train loss : 0.5060015618801117, validation loss : 0.6920011639595032\n",
      "Epoch: 25/300, train loss : 0.5040175914764404, validation loss : 0.7262137532234192\n",
      "Epoch: 26/300, train loss : 0.49246397614479065, validation loss : 0.7456848621368408\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4936208203434944, validation loss : 0.7501224875450134\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4827602580189705, validation loss : 0.7325168251991272\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.4589689001441002, validation loss : 0.7781880497932434\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4659763127565384, validation loss : 0.781225860118866\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.44851191341876984, validation loss : 0.7882788777351379\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.44712020456790924, validation loss : 0.7817221879959106\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.429989755153656, validation loss : 0.8498980402946472\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.43597714602947235, validation loss : 0.8433801531791687\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.42631153017282486, validation loss : 0.8200812935829163\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.4041227027773857, validation loss : 0.8710699081420898\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.40411484241485596, validation loss : 0.8866640329360962\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.384122759103775, validation loss : 0.9281764030456543\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.38925618678331375, validation loss : 0.9366343021392822\n",
      "Early stopping...\n",
      "bce:0.7325168251991272, acc :0.6883116883116883, f1: 0.6974789915966385, roc_auc: 0.7351351351351352\n",
      "bce:0.46437178552150726, acc :0.7887178308823529, f1: 0.831439393939394, roc_auc: 0.8520979770979772\n",
      "Rep no 1, Fold no 0\n",
      "Epoch: 1/300, train loss : 1.35731603205204, validation loss : 0.6672883033752441\n",
      "Epoch: 2/300, train loss : 0.7439104169607162, validation loss : 0.662447452545166\n",
      "Epoch: 3/300, train loss : 0.7037114202976227, validation loss : 0.7322527766227722\n",
      "Epoch: 4/300, train loss : 0.686347246170044, validation loss : 0.6495229601860046\n",
      "Epoch: 5/300, train loss : 0.6630199402570724, validation loss : 0.6638041138648987\n",
      "Epoch: 6/300, train loss : 0.6509682834148407, validation loss : 0.6500521898269653\n",
      "Epoch: 7/300, train loss : 0.6347060203552246, validation loss : 0.604597806930542\n",
      "Epoch: 8/300, train loss : 0.6238632500171661, validation loss : 0.6014937162399292\n",
      "Epoch: 9/300, train loss : 0.6110529154539108, validation loss : 0.6022516489028931\n",
      "Epoch: 10/300, train loss : 0.6115656942129135, validation loss : 0.5990172624588013\n",
      "Epoch: 11/300, train loss : 0.5970522612333298, validation loss : 0.6054405570030212\n",
      "Epoch: 12/300, train loss : 0.5989829897880554, validation loss : 0.6027153730392456\n",
      "Epoch: 13/300, train loss : 0.5871794819831848, validation loss : 0.5988261699676514\n",
      "Epoch: 14/300, train loss : 0.5855236798524857, validation loss : 0.5940647721290588\n",
      "Epoch: 15/300, train loss : 0.583365261554718, validation loss : 0.5918386578559875\n",
      "Epoch: 16/300, train loss : 0.5750301033258438, validation loss : 0.5993939638137817\n",
      "Epoch: 17/300, train loss : 0.5630147457122803, validation loss : 0.589553713798523\n",
      "Epoch: 18/300, train loss : 0.5547798424959183, validation loss : 0.6026828289031982\n",
      "Epoch: 19/300, train loss : 0.5450183302164078, validation loss : 0.5788536071777344\n",
      "Epoch: 20/300, train loss : 0.5368911623954773, validation loss : 0.5939557552337646\n",
      "Epoch: 21/300, train loss : 0.5409060716629028, validation loss : 0.5880518555641174\n",
      "Epoch: 22/300, train loss : 0.5523285418748856, validation loss : 0.5850736498832703\n",
      "Epoch: 23/300, train loss : 0.5433181822299957, validation loss : 0.6408283114433289\n",
      "Epoch: 24/300, train loss : 0.5254117697477341, validation loss : 0.5868932008743286\n",
      "Epoch: 25/300, train loss : 0.5162529796361923, validation loss : 0.6328358054161072\n",
      "Epoch: 26/300, train loss : 0.5035595148801804, validation loss : 0.597841739654541\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.5032058581709862, validation loss : 0.5916720628738403\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.4987892657518387, validation loss : 0.657334566116333\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.49568144977092743, validation loss : 0.6080034971237183\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4755861312150955, validation loss : 0.642242968082428\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.4699314087629318, validation loss : 0.6362007856369019\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.4709193781018257, validation loss : 0.6334046721458435\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.45799509435892105, validation loss : 0.6357053518295288\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4531533792614937, validation loss : 0.662865936756134\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4554241746664047, validation loss : 0.6707208752632141\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.4392913803458214, validation loss : 0.6425985097885132\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.4218999445438385, validation loss : 0.6500906944274902\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.4242703765630722, validation loss : 0.6665855646133423\n",
      "Early stopping...\n",
      "bce:0.5916720628738403, acc :0.6982758620689655, f1: 0.7865853658536586, roc_auc: 0.7077429149797572\n",
      "bce:0.4271664470434189, acc :0.8103170955882353, f1: 0.8474012179577963, roc_auc: 0.8774667524667525\n",
      "Rep no 1, Fold no 1\n",
      "Epoch: 1/300, train loss : 1.2760127484798431, validation loss : 0.678240954875946\n",
      "Epoch: 2/300, train loss : 0.7270723730325699, validation loss : 0.6882150769233704\n",
      "Epoch: 3/300, train loss : 0.6953273415565491, validation loss : 0.7463355660438538\n",
      "Epoch: 4/300, train loss : 0.6708378940820694, validation loss : 0.6577580571174622\n",
      "Epoch: 5/300, train loss : 0.6570691764354706, validation loss : 0.6461412906646729\n",
      "Epoch: 6/300, train loss : 0.6288012564182281, validation loss : 0.6492577791213989\n",
      "Epoch: 7/300, train loss : 0.6158763468265533, validation loss : 0.6288971900939941\n",
      "Epoch: 8/300, train loss : 0.6109582334756851, validation loss : 0.6173462867736816\n",
      "Epoch: 9/300, train loss : 0.5968866348266602, validation loss : 0.6246659755706787\n",
      "Epoch: 10/300, train loss : 0.5931503027677536, validation loss : 0.6286830902099609\n",
      "Epoch: 11/300, train loss : 0.579763650894165, validation loss : 0.6219476461410522\n",
      "Epoch: 12/300, train loss : 0.5760730654001236, validation loss : 0.6224268078804016\n",
      "Epoch: 13/300, train loss : 0.5666718780994415, validation loss : 0.6270098090171814\n",
      "Epoch: 14/300, train loss : 0.5624667555093765, validation loss : 0.6224157214164734\n",
      "Epoch: 15/300, train loss : 0.5566799491643906, validation loss : 0.6278674602508545\n",
      "Epoch: 16/300, train loss : 0.5510687977075577, validation loss : 0.6352591514587402\n",
      "Epoch: 17/300, train loss : 0.538472518324852, validation loss : 0.6259015798568726\n",
      "Epoch: 18/300, train loss : 0.5337280482053757, validation loss : 0.636991560459137\n",
      "Epoch: 19/300, train loss : 0.519081361591816, validation loss : 0.6368536949157715\n",
      "Epoch: 20/300, train loss : 0.5062851905822754, validation loss : 0.6597750782966614\n",
      "Epoch: 21/300, train loss : 0.5164580270648003, validation loss : 0.650627613067627\n",
      "Epoch: 22/300, train loss : 0.5131743550300598, validation loss : 0.6536963582038879\n",
      "Epoch: 23/300, train loss : 0.509813591837883, validation loss : 0.6582782864570618\n",
      "Epoch: 24/300, train loss : 0.49448031187057495, validation loss : 0.6594822406768799\n",
      "Epoch: 25/300, train loss : 0.4794348329305649, validation loss : 0.6965286135673523\n",
      "Epoch: 26/300, train loss : 0.4742550402879715, validation loss : 0.6758450865745544\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46318864822387695, validation loss : 0.6949132680892944\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.45719995349645615, validation loss : 0.688002347946167\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.44960591197013855, validation loss : 0.7014028429985046\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.4354999288916588, validation loss : 0.7100173830986023\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.4329524263739586, validation loss : 0.7063307166099548\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.4257645383477211, validation loss : 0.7253702282905579\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.4245270788669586, validation loss : 0.7320029139518738\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.40456756204366684, validation loss : 0.7224246859550476\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.40007560700178146, validation loss : 0.7600817680358887\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.39789261668920517, validation loss : 0.7819092273712158\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.37457234412431717, validation loss : 0.768003523349762\n",
      "Early stopping...\n",
      "bce:0.6758450865745544, acc :0.6422413793103449, f1: 0.7365079365079366, roc_auc: 0.6216886754701881\n",
      "bce:0.44534389674663544, acc :0.8308823529411764, f1: 0.8641025641025641, roc_auc: 0.8484222546722546\n",
      "Rep no 1, Fold no 2\n",
      "Epoch: 1/300, train loss : 1.3414556831121445, validation loss : 0.761675238609314\n",
      "Epoch: 2/300, train loss : 0.7178567498922348, validation loss : 0.7416431903839111\n",
      "Epoch: 3/300, train loss : 0.6655273884534836, validation loss : 0.6929261684417725\n",
      "Epoch: 4/300, train loss : 0.661252573132515, validation loss : 0.6823320388793945\n",
      "Epoch: 5/300, train loss : 0.6455592662096024, validation loss : 0.6942253708839417\n",
      "Epoch: 6/300, train loss : 0.618992030620575, validation loss : 0.6628371477127075\n",
      "Epoch: 7/300, train loss : 0.6030757874250412, validation loss : 0.6637211441993713\n",
      "Epoch: 8/300, train loss : 0.5999104082584381, validation loss : 0.6703303456306458\n",
      "Epoch: 9/300, train loss : 0.5871231108903885, validation loss : 0.6659092903137207\n",
      "Epoch: 10/300, train loss : 0.5825853943824768, validation loss : 0.6614685654640198\n",
      "Epoch: 11/300, train loss : 0.5748243778944016, validation loss : 0.659085214138031\n",
      "Epoch: 12/300, train loss : 0.5621080845594406, validation loss : 0.6628087759017944\n",
      "Epoch: 13/300, train loss : 0.5599378496408463, validation loss : 0.6723792552947998\n",
      "Epoch: 14/300, train loss : 0.5502545535564423, validation loss : 0.6751833558082581\n",
      "Epoch: 15/300, train loss : 0.549363523721695, validation loss : 0.6683082580566406\n",
      "Epoch: 16/300, train loss : 0.539231076836586, validation loss : 0.6680033802986145\n",
      "Epoch: 17/300, train loss : 0.5382298082113266, validation loss : 0.6718462705612183\n",
      "Epoch: 18/300, train loss : 0.5258362591266632, validation loss : 0.6612778902053833\n",
      "Epoch: 19/300, train loss : 0.5220088884234428, validation loss : 0.6614934802055359\n",
      "Epoch: 20/300, train loss : 0.5079383105039597, validation loss : 0.6732481718063354\n",
      "Epoch: 21/300, train loss : 0.5162286460399628, validation loss : 0.6845484375953674\n",
      "Epoch: 22/300, train loss : 0.5051885694265366, validation loss : 0.6831933259963989\n",
      "Epoch: 23/300, train loss : 0.49808187782764435, validation loss : 0.6784781813621521\n",
      "Epoch: 24/300, train loss : 0.48687468469142914, validation loss : 0.6724622249603271\n",
      "Epoch: 25/300, train loss : 0.47581495344638824, validation loss : 0.6759239435195923\n",
      "Epoch: 26/300, train loss : 0.469474196434021, validation loss : 0.6954784393310547\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46539729088544846, validation loss : 0.694714367389679\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44882354885339737, validation loss : 0.6912128329277039\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.44354572147130966, validation loss : 0.6986061334609985\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4393515810370445, validation loss : 0.7164421677589417\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.4199075922369957, validation loss : 0.712190568447113\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.41967616975307465, validation loss : 0.7465937733650208\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.40700873732566833, validation loss : 0.7328088283538818\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.39918361604213715, validation loss : 0.7588928937911987\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.3975133076310158, validation loss : 0.7462399005889893\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.3706049397587776, validation loss : 0.7957026362419128\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.3686867207288742, validation loss : 0.7793653011322021\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.39841316640377045, validation loss : 0.8056358098983765\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.36381692439317703, validation loss : 0.7984771728515625\n",
      "Early stopping...\n",
      "bce:0.6912128329277039, acc :0.6017316017316018, f1: 0.5964912280701754, roc_auc: 0.644442777861107\n",
      "bce:0.4824090451002121, acc :0.7877987132352942, f1: 0.833852544132918, roc_auc: 0.8351122413622414\n",
      "Rep no 1, Fold no 3\n",
      "Epoch: 1/300, train loss : 1.3191308081150055, validation loss : 0.8232491612434387\n",
      "Epoch: 2/300, train loss : 0.7354132235050201, validation loss : 0.8153864741325378\n",
      "Epoch: 3/300, train loss : 0.6752773076295853, validation loss : 0.6707612872123718\n",
      "Epoch: 4/300, train loss : 0.6795428842306137, validation loss : 0.6690120697021484\n",
      "Epoch: 5/300, train loss : 0.6380302459001541, validation loss : 0.7237035036087036\n",
      "Epoch: 6/300, train loss : 0.6218431144952774, validation loss : 0.692861795425415\n",
      "Epoch: 7/300, train loss : 0.6037435233592987, validation loss : 0.6510315537452698\n",
      "Epoch: 8/300, train loss : 0.5948467552661896, validation loss : 0.6560871005058289\n",
      "Epoch: 9/300, train loss : 0.586534708738327, validation loss : 0.676819920539856\n",
      "Epoch: 10/300, train loss : 0.5791041404008865, validation loss : 0.680763304233551\n",
      "Epoch: 11/300, train loss : 0.5755188763141632, validation loss : 0.6518516540527344\n",
      "Epoch: 12/300, train loss : 0.5616065263748169, validation loss : 0.6441763043403625\n",
      "Epoch: 13/300, train loss : 0.5543117374181747, validation loss : 0.6596513986587524\n",
      "Epoch: 14/300, train loss : 0.550444170832634, validation loss : 0.6657654643058777\n",
      "Epoch: 15/300, train loss : 0.5375259518623352, validation loss : 0.6496838331222534\n",
      "Epoch: 16/300, train loss : 0.5420093685388565, validation loss : 0.6687654852867126\n",
      "Epoch: 17/300, train loss : 0.5248028934001923, validation loss : 0.6729865670204163\n",
      "Epoch: 18/300, train loss : 0.5184603780508041, validation loss : 0.6614536046981812\n",
      "Epoch: 19/300, train loss : 0.5112252682447433, validation loss : 0.6709498167037964\n",
      "Epoch: 20/300, train loss : 0.502378948032856, validation loss : 0.6751149296760559\n",
      "Epoch: 21/300, train loss : 0.5091797262430191, validation loss : 0.667108952999115\n",
      "Epoch: 22/300, train loss : 0.5016944408416748, validation loss : 0.678236722946167\n",
      "Epoch: 23/300, train loss : 0.48483406752347946, validation loss : 0.6993914246559143\n",
      "Epoch: 24/300, train loss : 0.48425229638814926, validation loss : 0.6837584972381592\n",
      "Epoch: 25/300, train loss : 0.4748079776763916, validation loss : 0.6902567148208618\n",
      "Epoch: 26/300, train loss : 0.46514227986335754, validation loss : 0.7112748026847839\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4572925865650177, validation loss : 0.7030712366104126\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44872669130563736, validation loss : 0.7194381356239319\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.4332929477095604, validation loss : 0.729004979133606\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4233749210834503, validation loss : 0.734040379524231\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.41883385181427, validation loss : 0.7270990014076233\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.4177447706460953, validation loss : 0.7312628626823425\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.4010084643959999, validation loss : 0.7803354859352112\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.41193149238824844, validation loss : 0.7599702477455139\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.40641331672668457, validation loss : 0.7961417436599731\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.38078854978084564, validation loss : 0.7700895667076111\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.37055449932813644, validation loss : 0.8057439923286438\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.37510692328214645, validation loss : 0.7699239253997803\n",
      "Early stopping...\n",
      "bce:0.7030712366104126, acc :0.5887445887445888, f1: 0.46327683615819215, roc_auc: 0.6342599782574934\n",
      "bce:0.4291783273220062, acc :0.8064108455882353, f1: 0.8399275143461189, roc_auc: 0.8676210551210551\n",
      "Rep no 1, Fold no 4\n",
      "Epoch: 1/300, train loss : 1.4181728512048721, validation loss : 0.7921507954597473\n",
      "Epoch: 2/300, train loss : 0.7452310472726822, validation loss : 0.7313492894172668\n",
      "Epoch: 3/300, train loss : 0.6881860792636871, validation loss : 0.6833483576774597\n",
      "Epoch: 4/300, train loss : 0.683175727725029, validation loss : 0.6777384877204895\n",
      "Epoch: 5/300, train loss : 0.6706948727369308, validation loss : 0.6854179501533508\n",
      "Epoch: 6/300, train loss : 0.6394778043031693, validation loss : 0.6372373104095459\n",
      "Epoch: 7/300, train loss : 0.6235809922218323, validation loss : 0.6322788000106812\n",
      "Epoch: 8/300, train loss : 0.6165443360805511, validation loss : 0.6307584643363953\n",
      "Epoch: 9/300, train loss : 0.6128071546554565, validation loss : 0.6272329092025757\n",
      "Epoch: 10/300, train loss : 0.6024026870727539, validation loss : 0.6282917857170105\n",
      "Epoch: 11/300, train loss : 0.5982252657413483, validation loss : 0.6256157755851746\n",
      "Epoch: 12/300, train loss : 0.5868625491857529, validation loss : 0.6253120303153992\n",
      "Epoch: 13/300, train loss : 0.5753292441368103, validation loss : 0.6338567733764648\n",
      "Epoch: 14/300, train loss : 0.5797981321811676, validation loss : 0.6372199654579163\n",
      "Epoch: 15/300, train loss : 0.5687081813812256, validation loss : 0.638832151889801\n",
      "Epoch: 16/300, train loss : 0.563472256064415, validation loss : 0.6612160801887512\n",
      "Epoch: 17/300, train loss : 0.5548782050609589, validation loss : 0.6509159207344055\n",
      "Epoch: 18/300, train loss : 0.5507961958646774, validation loss : 0.6562729477882385\n",
      "Epoch: 19/300, train loss : 0.5431348234415054, validation loss : 0.6633772253990173\n",
      "Epoch: 20/300, train loss : 0.5330420881509781, validation loss : 0.6659421324729919\n",
      "Epoch: 21/300, train loss : 0.5311876833438873, validation loss : 0.6739058494567871\n",
      "Epoch: 22/300, train loss : 0.5285270661115646, validation loss : 0.6845971345901489\n",
      "Epoch: 23/300, train loss : 0.5055114105343819, validation loss : 0.6947527527809143\n",
      "Epoch: 24/300, train loss : 0.5064904242753983, validation loss : 0.6886498332023621\n",
      "Epoch: 25/300, train loss : 0.5044620186090469, validation loss : 0.7228080630302429\n",
      "Epoch: 26/300, train loss : 0.4932986721396446, validation loss : 0.7430688142776489\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.49342796206474304, validation loss : 0.7501980066299438\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4828941747546196, validation loss : 0.7284950613975525\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.4593883529305458, validation loss : 0.7755078077316284\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.466655470430851, validation loss : 0.7745932936668396\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.4492816627025604, validation loss : 0.7897405028343201\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.4483680799603462, validation loss : 0.7764319181442261\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.43217670172452927, validation loss : 0.841896116733551\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.43713661283254623, validation loss : 0.8336909413337708\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.42666690796613693, validation loss : 0.8167147040367126\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.40526823699474335, validation loss : 0.8732511401176453\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.4041493237018585, validation loss : 0.8870664834976196\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.3868931010365486, validation loss : 0.9253028631210327\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.3908947929739952, validation loss : 0.9380687475204468\n",
      "Early stopping...\n",
      "bce:0.7284950613975525, acc :0.696969696969697, f1: 0.7033898305084746, roc_auc: 0.7349849849849849\n",
      "bce:0.4660293757915497, acc :0.7887178308823529, f1: 0.8307203389830509, roc_auc: 0.8476842226842227\n",
      "Rep no 2, Fold no 0\n",
      "Epoch: 1/300, train loss : 1.3573159724473953, validation loss : 0.6672883033752441\n",
      "Epoch: 2/300, train loss : 0.7439104169607162, validation loss : 0.662447452545166\n",
      "Epoch: 3/300, train loss : 0.7037114202976227, validation loss : 0.7322527766227722\n",
      "Epoch: 4/300, train loss : 0.6863479316234589, validation loss : 0.649482250213623\n",
      "Epoch: 5/300, train loss : 0.6630380004644394, validation loss : 0.6638325452804565\n",
      "Epoch: 6/300, train loss : 0.6510284096002579, validation loss : 0.6501356959342957\n",
      "Epoch: 7/300, train loss : 0.6347385346889496, validation loss : 0.6045759916305542\n",
      "Epoch: 8/300, train loss : 0.623915895819664, validation loss : 0.601349413394928\n",
      "Epoch: 9/300, train loss : 0.6111623048782349, validation loss : 0.6026980876922607\n",
      "Epoch: 10/300, train loss : 0.6115440279245377, validation loss : 0.5985709428787231\n",
      "Epoch: 11/300, train loss : 0.59674072265625, validation loss : 0.6055896282196045\n",
      "Epoch: 12/300, train loss : 0.5989644676446915, validation loss : 0.6023347973823547\n",
      "Epoch: 13/300, train loss : 0.5870146006345749, validation loss : 0.5990944504737854\n",
      "Epoch: 14/300, train loss : 0.5853649377822876, validation loss : 0.5944559574127197\n",
      "Epoch: 15/300, train loss : 0.5831655263900757, validation loss : 0.5929869413375854\n",
      "Epoch: 16/300, train loss : 0.5750310271978378, validation loss : 0.6017162799835205\n",
      "Epoch: 17/300, train loss : 0.5630531758069992, validation loss : 0.5891618132591248\n",
      "Epoch: 18/300, train loss : 0.5545591562986374, validation loss : 0.6030199527740479\n",
      "Epoch: 19/300, train loss : 0.5449804365634918, validation loss : 0.5811450481414795\n",
      "Epoch: 20/300, train loss : 0.536555714905262, validation loss : 0.5943654775619507\n",
      "Epoch: 21/300, train loss : 0.5407129377126694, validation loss : 0.587904155254364\n",
      "Epoch: 22/300, train loss : 0.5514979213476181, validation loss : 0.588008463382721\n",
      "Epoch: 23/300, train loss : 0.5441916137933731, validation loss : 0.6383486986160278\n",
      "Epoch: 24/300, train loss : 0.5263801366090775, validation loss : 0.5889375805854797\n",
      "Epoch: 25/300, train loss : 0.5166704952716827, validation loss : 0.6378098726272583\n",
      "Epoch: 26/300, train loss : 0.5037155076861382, validation loss : 0.5919573307037354\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.5036000609397888, validation loss : 0.5902696251869202\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.49966834485530853, validation loss : 0.6606113910675049\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.49675463140010834, validation loss : 0.6038571000099182\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4766010120511055, validation loss : 0.6361786127090454\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.4716256856918335, validation loss : 0.6297003626823425\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.46994824707508087, validation loss : 0.6325646638870239\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.45935945957899094, validation loss : 0.6276078224182129\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4527757838368416, validation loss : 0.6608055830001831\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.45625779032707214, validation loss : 0.6608981490135193\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.4403409883379936, validation loss : 0.6428742408752441\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.4220967963337898, validation loss : 0.6462954878807068\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.424401618540287, validation loss : 0.6545552611351013\n",
      "Early stopping...\n",
      "bce:0.5902696251869202, acc :0.7025862068965517, f1: 0.7889908256880734, roc_auc: 0.707152496626181\n",
      "bce:0.42747481167316437, acc :0.8044577205882353, f1: 0.8413901601830664, roc_auc: 0.8755414067914068\n",
      "Rep no 2, Fold no 1\n",
      "Epoch: 1/300, train loss : 1.2760127633810043, validation loss : 0.678240954875946\n",
      "Epoch: 2/300, train loss : 0.7270723581314087, validation loss : 0.6882150769233704\n",
      "Epoch: 3/300, train loss : 0.6953273564577103, validation loss : 0.7463356256484985\n",
      "Epoch: 4/300, train loss : 0.6708378940820694, validation loss : 0.6577580571174622\n",
      "Epoch: 5/300, train loss : 0.6570691913366318, validation loss : 0.6461412906646729\n",
      "Epoch: 6/300, train loss : 0.6288012564182281, validation loss : 0.6492577791213989\n",
      "Epoch: 7/300, train loss : 0.6158763021230698, validation loss : 0.6288970112800598\n",
      "Epoch: 8/300, train loss : 0.6109587550163269, validation loss : 0.6173441410064697\n",
      "Epoch: 9/300, train loss : 0.5968872755765915, validation loss : 0.6246511340141296\n",
      "Epoch: 10/300, train loss : 0.5931415408849716, validation loss : 0.6286243796348572\n",
      "Epoch: 11/300, train loss : 0.5797558724880219, validation loss : 0.6215649247169495\n",
      "Epoch: 12/300, train loss : 0.5759979039430618, validation loss : 0.6223893165588379\n",
      "Epoch: 13/300, train loss : 0.5668779164552689, validation loss : 0.6269465684890747\n",
      "Epoch: 14/300, train loss : 0.5623587965965271, validation loss : 0.622270405292511\n",
      "Epoch: 15/300, train loss : 0.5567089915275574, validation loss : 0.6279504895210266\n",
      "Epoch: 16/300, train loss : 0.5510468035936356, validation loss : 0.635028600692749\n",
      "Epoch: 17/300, train loss : 0.5384943783283234, validation loss : 0.6257666349411011\n",
      "Epoch: 18/300, train loss : 0.533621147274971, validation loss : 0.6367214918136597\n",
      "Epoch: 19/300, train loss : 0.5190697833895683, validation loss : 0.6366016268730164\n",
      "Epoch: 20/300, train loss : 0.5059373676776886, validation loss : 0.6609629392623901\n",
      "Epoch: 21/300, train loss : 0.516414076089859, validation loss : 0.6529958248138428\n",
      "Epoch: 22/300, train loss : 0.5131192654371262, validation loss : 0.6552980542182922\n",
      "Epoch: 23/300, train loss : 0.5091129243373871, validation loss : 0.6592075824737549\n",
      "Epoch: 24/300, train loss : 0.49378247559070587, validation loss : 0.6610705852508545\n",
      "Epoch: 25/300, train loss : 0.4789644554257393, validation loss : 0.6963299512863159\n",
      "Epoch: 26/300, train loss : 0.47368675470352173, validation loss : 0.6759008765220642\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.463829405605793, validation loss : 0.6972202658653259\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.45619580894708633, validation loss : 0.687216579914093\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.44853300601243973, validation loss : 0.7032233476638794\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.4350687637925148, validation loss : 0.7124413847923279\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.43283985555171967, validation loss : 0.7070485949516296\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.4249069392681122, validation loss : 0.7248774170875549\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.4242653325200081, validation loss : 0.732406497001648\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.40517961978912354, validation loss : 0.7205113172531128\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.4011918008327484, validation loss : 0.7569566369056702\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.39901003241539, validation loss : 0.7732658982276917\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.3740895241498947, validation loss : 0.766947329044342\n",
      "Early stopping...\n",
      "bce:0.6759008169174194, acc :0.6336206896551724, f1: 0.7301587301587302, roc_auc: 0.6204881952781112\n",
      "bce:0.44698813557624817, acc :0.8269761029411764, f1: 0.8608974358974358, roc_auc: 0.8478591916091915\n",
      "Rep no 2, Fold no 2\n",
      "Epoch: 1/300, train loss : 1.3414556831121445, validation loss : 0.761675238609314\n",
      "Epoch: 2/300, train loss : 0.7178567498922348, validation loss : 0.7416423559188843\n",
      "Epoch: 3/300, train loss : 0.6655280441045761, validation loss : 0.6929364800453186\n",
      "Epoch: 4/300, train loss : 0.6612863689661026, validation loss : 0.6823404431343079\n",
      "Epoch: 5/300, train loss : 0.6455519944429398, validation loss : 0.6942251324653625\n",
      "Epoch: 6/300, train loss : 0.6189363151788712, validation loss : 0.6628661155700684\n",
      "Epoch: 7/300, train loss : 0.6031247526407242, validation loss : 0.6636863946914673\n",
      "Epoch: 8/300, train loss : 0.5998344123363495, validation loss : 0.6700555682182312\n",
      "Epoch: 9/300, train loss : 0.5870343148708344, validation loss : 0.6654241681098938\n",
      "Epoch: 10/300, train loss : 0.5825431793928146, validation loss : 0.6607716679573059\n",
      "Epoch: 11/300, train loss : 0.5748042166233063, validation loss : 0.6587203145027161\n",
      "Epoch: 12/300, train loss : 0.5623109340667725, validation loss : 0.6622377634048462\n",
      "Epoch: 13/300, train loss : 0.5597216039896011, validation loss : 0.6729039549827576\n",
      "Epoch: 14/300, train loss : 0.5505092591047287, validation loss : 0.6760880351066589\n",
      "Epoch: 15/300, train loss : 0.5493187010288239, validation loss : 0.6693286895751953\n",
      "Epoch: 16/300, train loss : 0.539027750492096, validation loss : 0.6684735417366028\n",
      "Epoch: 17/300, train loss : 0.5380232185125351, validation loss : 0.673005223274231\n",
      "Epoch: 18/300, train loss : 0.5258372351527214, validation loss : 0.6619364023208618\n",
      "Epoch: 19/300, train loss : 0.5218297019600868, validation loss : 0.6627312302589417\n",
      "Epoch: 20/300, train loss : 0.5085750296711922, validation loss : 0.6732222437858582\n",
      "Epoch: 21/300, train loss : 0.5165917947888374, validation loss : 0.6846532225608826\n",
      "Epoch: 22/300, train loss : 0.5056591555476189, validation loss : 0.6836970448493958\n",
      "Epoch: 23/300, train loss : 0.4991842582821846, validation loss : 0.6783232092857361\n",
      "Epoch: 24/300, train loss : 0.4880855530500412, validation loss : 0.6708944439888\n",
      "Epoch: 25/300, train loss : 0.4755531772971153, validation loss : 0.6761418581008911\n",
      "Epoch: 26/300, train loss : 0.4707197770476341, validation loss : 0.6974402666091919\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4649442285299301, validation loss : 0.6953878402709961\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44989703595638275, validation loss : 0.6923390030860901\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.4433664381504059, validation loss : 0.7004422545433044\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4396738186478615, validation loss : 0.7187724709510803\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.42078959196805954, validation loss : 0.7126262784004211\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.42144906520843506, validation loss : 0.7438061237335205\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.40804871171712875, validation loss : 0.7306529879570007\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.4000651240348816, validation loss : 0.7492558360099792\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.3979717716574669, validation loss : 0.745673418045044\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.37133803218603134, validation loss : 0.7977041006088257\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.36808642745018005, validation loss : 0.7785379886627197\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.3963015154004097, validation loss : 0.8063735365867615\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.36242087185382843, validation loss : 0.7934717535972595\n",
      "Early stopping...\n",
      "bce:0.6923390030860901, acc :0.6060606060606061, f1: 0.6026200873362445, roc_auc: 0.6404679766011699\n",
      "bce:0.4831231087446213, acc :0.7897518382352942, f1: 0.8343815513626835, roc_auc: 0.8339548339548339\n",
      "Rep no 2, Fold no 3\n",
      "Epoch: 1/300, train loss : 1.3191308081150055, validation loss : 0.8232491612434387\n",
      "Epoch: 2/300, train loss : 0.7354132384061813, validation loss : 0.8153864741325378\n",
      "Epoch: 3/300, train loss : 0.6752772778272629, validation loss : 0.6707612872123718\n",
      "Epoch: 4/300, train loss : 0.679545983672142, validation loss : 0.6689958572387695\n",
      "Epoch: 5/300, train loss : 0.6380354315042496, validation loss : 0.7236869931221008\n",
      "Epoch: 6/300, train loss : 0.6218600869178772, validation loss : 0.6928249597549438\n",
      "Epoch: 7/300, train loss : 0.6037453413009644, validation loss : 0.6510027050971985\n",
      "Epoch: 8/300, train loss : 0.5947704911231995, validation loss : 0.6561669111251831\n",
      "Epoch: 9/300, train loss : 0.5865524560213089, validation loss : 0.6768494844436646\n",
      "Epoch: 10/300, train loss : 0.5790760070085526, validation loss : 0.6802778840065002\n",
      "Epoch: 11/300, train loss : 0.5753712207078934, validation loss : 0.6511468291282654\n",
      "Epoch: 12/300, train loss : 0.5615150034427643, validation loss : 0.6443515419960022\n",
      "Epoch: 13/300, train loss : 0.5540417432785034, validation loss : 0.6601427793502808\n",
      "Epoch: 14/300, train loss : 0.5501843094825745, validation loss : 0.6646214127540588\n",
      "Epoch: 15/300, train loss : 0.537371076643467, validation loss : 0.649502694606781\n",
      "Epoch: 16/300, train loss : 0.5417349636554718, validation loss : 0.6704954504966736\n",
      "Epoch: 17/300, train loss : 0.5247059911489487, validation loss : 0.6737516522407532\n",
      "Epoch: 18/300, train loss : 0.5181793123483658, validation loss : 0.662442147731781\n",
      "Epoch: 19/300, train loss : 0.5109462440013885, validation loss : 0.672863245010376\n",
      "Epoch: 20/300, train loss : 0.5025004297494888, validation loss : 0.6757572293281555\n",
      "Epoch: 21/300, train loss : 0.5088400319218636, validation loss : 0.6692075133323669\n",
      "Epoch: 22/300, train loss : 0.50174630433321, validation loss : 0.6780961155891418\n",
      "Epoch: 23/300, train loss : 0.4846690446138382, validation loss : 0.695831298828125\n",
      "Epoch: 24/300, train loss : 0.4842091202735901, validation loss : 0.6831337213516235\n",
      "Epoch: 25/300, train loss : 0.47496404498815536, validation loss : 0.6875713467597961\n",
      "Epoch: 26/300, train loss : 0.464946024119854, validation loss : 0.710221529006958\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.45813369005918503, validation loss : 0.6970540285110474\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44801629334688187, validation loss : 0.7173300981521606\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.43424102663993835, validation loss : 0.7203648090362549\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4238125905394554, validation loss : 0.7331646680831909\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.419335700571537, validation loss : 0.7235099673271179\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.4195297658443451, validation loss : 0.725713849067688\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.40293510258197784, validation loss : 0.7864755988121033\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4162773862481117, validation loss : 0.7598439455032349\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.40768834948539734, validation loss : 0.7807046175003052\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.3831397444009781, validation loss : 0.7605352997779846\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.3731376454234123, validation loss : 0.7970340847969055\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.3741373345255852, validation loss : 0.761379599571228\n",
      "Early stopping...\n",
      "bce:0.6970540285110474, acc :0.5974025974025974, f1: 0.4685714285714286, roc_auc: 0.6423357664233577\n",
      "bce:0.42519669234752655, acc :0.8103170955882353, f1: 0.8409861325115562, roc_auc: 0.8683405245905246\n",
      "Rep no 2, Fold no 4\n",
      "Epoch: 1/300, train loss : 1.418172910809517, validation loss : 0.7921507954597473\n",
      "Epoch: 2/300, train loss : 0.7452310472726822, validation loss : 0.7313492894172668\n",
      "Epoch: 3/300, train loss : 0.6881860643625259, validation loss : 0.6833483576774597\n",
      "Epoch: 4/300, train loss : 0.6831756979227066, validation loss : 0.6777384877204895\n",
      "Epoch: 5/300, train loss : 0.6706948727369308, validation loss : 0.6854179501533508\n",
      "Epoch: 6/300, train loss : 0.6394778043031693, validation loss : 0.6372374296188354\n",
      "Epoch: 7/300, train loss : 0.6235816031694412, validation loss : 0.6322792768478394\n",
      "Epoch: 8/300, train loss : 0.6165456771850586, validation loss : 0.6307623982429504\n",
      "Epoch: 9/300, train loss : 0.6128120571374893, validation loss : 0.6272515654563904\n",
      "Epoch: 10/300, train loss : 0.6024100482463837, validation loss : 0.6283546686172485\n",
      "Epoch: 11/300, train loss : 0.5982291102409363, validation loss : 0.6258130669593811\n",
      "Epoch: 12/300, train loss : 0.5869016200304031, validation loss : 0.6257528066635132\n",
      "Epoch: 13/300, train loss : 0.5752854198217392, validation loss : 0.6344029307365417\n",
      "Epoch: 14/300, train loss : 0.5798221528530121, validation loss : 0.637721598148346\n",
      "Epoch: 15/300, train loss : 0.5687263906002045, validation loss : 0.6394167542457581\n",
      "Epoch: 16/300, train loss : 0.5635055750608444, validation loss : 0.6616355180740356\n",
      "Epoch: 17/300, train loss : 0.5548105835914612, validation loss : 0.6522054076194763\n",
      "Epoch: 18/300, train loss : 0.5507409423589706, validation loss : 0.6575126051902771\n",
      "Epoch: 19/300, train loss : 0.5427483320236206, validation loss : 0.6639330387115479\n",
      "Epoch: 20/300, train loss : 0.5329082310199738, validation loss : 0.6661111116409302\n",
      "Epoch: 21/300, train loss : 0.5318147987127304, validation loss : 0.6744387149810791\n",
      "Epoch: 22/300, train loss : 0.5287977159023285, validation loss : 0.6843535900115967\n",
      "Epoch: 23/300, train loss : 0.5056744515895844, validation loss : 0.695265531539917\n",
      "Epoch: 24/300, train loss : 0.5063949376344681, validation loss : 0.6911737322807312\n",
      "Epoch: 25/300, train loss : 0.5044196099042892, validation loss : 0.7257639169692993\n",
      "Epoch: 26/300, train loss : 0.4930325224995613, validation loss : 0.7436882853507996\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4935401827096939, validation loss : 0.7510840892791748\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4831180199980736, validation loss : 0.7291569113731384\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.45928604155778885, validation loss : 0.7757571935653687\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4661860018968582, validation loss : 0.7800599336624146\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.44920872151851654, validation loss : 0.7899564504623413\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.44777216017246246, validation loss : 0.7803336381912231\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.4307055249810219, validation loss : 0.8451816439628601\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.43585947155952454, validation loss : 0.834077775478363\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.4256126508116722, validation loss : 0.819819986820221\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.40403836220502853, validation loss : 0.8696635961532593\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.4037396162748337, validation loss : 0.8846291303634644\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.3859732300043106, validation loss : 0.9298045039176941\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.38956450670957565, validation loss : 0.9384661912918091\n",
      "Early stopping...\n",
      "bce:0.7291569113731384, acc :0.696969696969697, f1: 0.7033898305084746, roc_auc: 0.7357357357357357\n",
      "bce:0.46486541628837585, acc :0.7926240808823529, f1: 0.8341101694915255, roc_auc: 0.8534602597102596\n",
      "Rep no 3, Fold no 0\n",
      "Epoch: 1/300, train loss : 1.3573159873485565, validation loss : 0.6672883033752441\n",
      "Epoch: 2/300, train loss : 0.7439103871583939, validation loss : 0.6624475717544556\n",
      "Epoch: 3/300, train loss : 0.7037111818790436, validation loss : 0.7322511076927185\n",
      "Epoch: 4/300, train loss : 0.6863258332014084, validation loss : 0.6494010090827942\n",
      "Epoch: 5/300, train loss : 0.6629908531904221, validation loss : 0.6637245416641235\n",
      "Epoch: 6/300, train loss : 0.6509633660316467, validation loss : 0.6499987244606018\n",
      "Epoch: 7/300, train loss : 0.634747326374054, validation loss : 0.6041989922523499\n",
      "Epoch: 8/300, train loss : 0.6238701790571213, validation loss : 0.6014638543128967\n",
      "Epoch: 9/300, train loss : 0.6110956966876984, validation loss : 0.6025854349136353\n",
      "Epoch: 10/300, train loss : 0.61154705286026, validation loss : 0.598860502243042\n",
      "Epoch: 11/300, train loss : 0.5964643955230713, validation loss : 0.6059418320655823\n",
      "Epoch: 12/300, train loss : 0.598840981721878, validation loss : 0.6027754545211792\n",
      "Epoch: 13/300, train loss : 0.5871472954750061, validation loss : 0.6002320051193237\n",
      "Epoch: 14/300, train loss : 0.585075244307518, validation loss : 0.5952861309051514\n",
      "Epoch: 15/300, train loss : 0.5827738046646118, validation loss : 0.5931580066680908\n",
      "Epoch: 16/300, train loss : 0.574070081114769, validation loss : 0.6015586853027344\n",
      "Epoch: 17/300, train loss : 0.5627982914447784, validation loss : 0.5927532911300659\n",
      "Epoch: 18/300, train loss : 0.5538668781518936, validation loss : 0.6017155647277832\n",
      "Epoch: 19/300, train loss : 0.5443763136863708, validation loss : 0.5803291201591492\n",
      "Epoch: 20/300, train loss : 0.5353270918130875, validation loss : 0.5968006253242493\n",
      "Epoch: 21/300, train loss : 0.5393328070640564, validation loss : 0.5840731859207153\n",
      "Epoch: 22/300, train loss : 0.5506327450275421, validation loss : 0.5903440713882446\n",
      "Epoch: 23/300, train loss : 0.5440790951251984, validation loss : 0.6359145641326904\n",
      "Epoch: 24/300, train loss : 0.5256477892398834, validation loss : 0.5860931873321533\n",
      "Epoch: 25/300, train loss : 0.516594186425209, validation loss : 0.6358242630958557\n",
      "Epoch: 26/300, train loss : 0.5038883909583092, validation loss : 0.5905410051345825\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.5023556426167488, validation loss : 0.5928958058357239\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4998430088162422, validation loss : 0.6585564613342285\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.49609769880771637, validation loss : 0.6044662594795227\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.4759756252169609, validation loss : 0.6399325728416443\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.47283919900655746, validation loss : 0.6288065910339355\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.46821361780166626, validation loss : 0.6315427422523499\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.4595850631594658, validation loss : 0.6356725096702576\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.45305386930704117, validation loss : 0.6615458726882935\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.45650404691696167, validation loss : 0.6625847816467285\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.4395964741706848, validation loss : 0.652728796005249\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.4227021411061287, validation loss : 0.6513241529464722\n",
      "Early stopping...\n",
      "bce:0.5905409455299377, acc :0.7068965517241379, f1: 0.7914110429447851, roc_auc: 0.7085020242914981\n",
      "bce:0.4342859238386154, acc :0.8005514705882353, f1: 0.8373164411171897, roc_auc: 0.8754960317460317\n",
      "Rep no 3, Fold no 1\n",
      "Epoch: 1/300, train loss : 1.2760126888751984, validation loss : 0.6782402396202087\n",
      "Epoch: 2/300, train loss : 0.7270725071430206, validation loss : 0.6882166266441345\n",
      "Epoch: 3/300, train loss : 0.6953309029340744, validation loss : 0.7463518977165222\n",
      "Epoch: 4/300, train loss : 0.6708426177501678, validation loss : 0.6577995419502258\n",
      "Epoch: 5/300, train loss : 0.6570775210857391, validation loss : 0.6462266445159912\n",
      "Epoch: 6/300, train loss : 0.6288086920976639, validation loss : 0.6492612361907959\n",
      "Epoch: 7/300, train loss : 0.6158531755208969, validation loss : 0.6289065480232239\n",
      "Epoch: 8/300, train loss : 0.6109486371278763, validation loss : 0.6172018647193909\n",
      "Epoch: 9/300, train loss : 0.5968352556228638, validation loss : 0.6245637536048889\n",
      "Epoch: 10/300, train loss : 0.5932634621858597, validation loss : 0.6293312311172485\n",
      "Epoch: 11/300, train loss : 0.5798996686935425, validation loss : 0.6220054626464844\n",
      "Epoch: 12/300, train loss : 0.5759752690792084, validation loss : 0.6221739649772644\n",
      "Epoch: 13/300, train loss : 0.5667715668678284, validation loss : 0.6265482306480408\n",
      "Epoch: 14/300, train loss : 0.562076672911644, validation loss : 0.6230391263961792\n",
      "Epoch: 15/300, train loss : 0.5562201738357544, validation loss : 0.6292517781257629\n",
      "Epoch: 16/300, train loss : 0.5504155158996582, validation loss : 0.63499516248703\n",
      "Epoch: 17/300, train loss : 0.5381808876991272, validation loss : 0.6276434063911438\n",
      "Epoch: 18/300, train loss : 0.5333510041236877, validation loss : 0.6388404369354248\n",
      "Epoch: 19/300, train loss : 0.518518477678299, validation loss : 0.638565719127655\n",
      "Epoch: 20/300, train loss : 0.5055408701300621, validation loss : 0.6609506011009216\n",
      "Epoch: 21/300, train loss : 0.514968141913414, validation loss : 0.6524447202682495\n",
      "Epoch: 22/300, train loss : 0.5119238570332527, validation loss : 0.6576138138771057\n",
      "Epoch: 23/300, train loss : 0.508319765329361, validation loss : 0.6582477688789368\n",
      "Epoch: 24/300, train loss : 0.49471454322338104, validation loss : 0.6623968482017517\n",
      "Epoch: 25/300, train loss : 0.4795040190219879, validation loss : 0.6976643800735474\n",
      "Epoch: 26/300, train loss : 0.47341492027044296, validation loss : 0.6763079762458801\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46418894827365875, validation loss : 0.701090395450592\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.45590896159410477, validation loss : 0.689163088798523\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.45014362037181854, validation loss : 0.7034833431243896\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.43606194853782654, validation loss : 0.7088239192962646\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.43357301503419876, validation loss : 0.7068347334861755\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.4261147230863571, validation loss : 0.7259920835494995\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.42433972656726837, validation loss : 0.7361659407615662\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.4049234539270401, validation loss : 0.7169941067695618\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.40127816796302795, validation loss : 0.7506031394004822\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.3997120037674904, validation loss : 0.7674413323402405\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.37310443818569183, validation loss : 0.7710050344467163\n",
      "Early stopping...\n",
      "bce:0.6763079762458801, acc :0.6422413793103449, f1: 0.7398119122257054, roc_auc: 0.6197679071628651\n",
      "bce:0.4490176737308502, acc :0.8328354779411764, f1: 0.8672975814931652, roc_auc: 0.8458400020900021\n",
      "Rep no 3, Fold no 2\n",
      "Epoch: 1/300, train loss : 1.3414556831121445, validation loss : 0.761675238609314\n",
      "Epoch: 2/300, train loss : 0.717856764793396, validation loss : 0.7416423559188843\n",
      "Epoch: 3/300, train loss : 0.6655282229185104, validation loss : 0.6929344534873962\n",
      "Epoch: 4/300, train loss : 0.6612602919340134, validation loss : 0.6823170185089111\n",
      "Epoch: 5/300, train loss : 0.6455835402011871, validation loss : 0.694117546081543\n",
      "Epoch: 6/300, train loss : 0.6189843863248825, validation loss : 0.6628841757774353\n",
      "Epoch: 7/300, train loss : 0.6030892878770828, validation loss : 0.6636629700660706\n",
      "Epoch: 8/300, train loss : 0.5998333096504211, validation loss : 0.6702941656112671\n",
      "Epoch: 9/300, train loss : 0.5870400369167328, validation loss : 0.6658841967582703\n",
      "Epoch: 10/300, train loss : 0.5824714303016663, validation loss : 0.6615105271339417\n",
      "Epoch: 11/300, train loss : 0.5746610164642334, validation loss : 0.6594439148902893\n",
      "Epoch: 12/300, train loss : 0.5620888769626617, validation loss : 0.6632579565048218\n",
      "Epoch: 13/300, train loss : 0.5597817748785019, validation loss : 0.6735199093818665\n",
      "Epoch: 14/300, train loss : 0.5502845644950867, validation loss : 0.6759277582168579\n",
      "Epoch: 15/300, train loss : 0.5495656877756119, validation loss : 0.668557345867157\n",
      "Epoch: 16/300, train loss : 0.5387581139802933, validation loss : 0.6672230958938599\n",
      "Epoch: 17/300, train loss : 0.5377453565597534, validation loss : 0.6697237491607666\n",
      "Epoch: 18/300, train loss : 0.5252568498253822, validation loss : 0.6602616906166077\n",
      "Epoch: 19/300, train loss : 0.5219746381044388, validation loss : 0.6619285345077515\n",
      "Epoch: 20/300, train loss : 0.5070311352610588, validation loss : 0.6733347177505493\n",
      "Epoch: 21/300, train loss : 0.5149813666939735, validation loss : 0.6848207712173462\n",
      "Epoch: 22/300, train loss : 0.5053377002477646, validation loss : 0.682538628578186\n",
      "Epoch: 23/300, train loss : 0.4979352578520775, validation loss : 0.6775364279747009\n",
      "Epoch: 24/300, train loss : 0.486772283911705, validation loss : 0.6706142425537109\n",
      "Epoch: 25/300, train loss : 0.47434524446725845, validation loss : 0.6760955452919006\n",
      "Epoch: 26/300, train loss : 0.4684859812259674, validation loss : 0.6964438557624817\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46425770223140717, validation loss : 0.6949694156646729\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44754642993211746, validation loss : 0.6898586750030518\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.44400936365127563, validation loss : 0.6943739652633667\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.43906648457050323, validation loss : 0.7150050401687622\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.41940728574991226, validation loss : 0.7124127149581909\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.41792958229780197, validation loss : 0.7498878240585327\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.40625136345624924, validation loss : 0.727900505065918\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.39805829524993896, validation loss : 0.7510355114936829\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.3951578810811043, validation loss : 0.7392570376396179\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.36852943897247314, validation loss : 0.7940658330917358\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.3660508096218109, validation loss : 0.7748918533325195\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.3933035582304001, validation loss : 0.8038924336433411\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.3608986809849739, validation loss : 0.8043113946914673\n",
      "Early stopping...\n",
      "bce:0.6898586750030518, acc :0.5974025974025974, f1: 0.5974025974025974, roc_auc: 0.6424928753562321\n",
      "bce:0.4843009412288666, acc :0.7936580882352942, f1: 0.8388543823326433, roc_auc: 0.8355048042548042\n",
      "Rep no 3, Fold no 3\n",
      "Epoch: 1/300, train loss : 1.319130778312683, validation loss : 0.8232491612434387\n",
      "Epoch: 2/300, train loss : 0.7354132384061813, validation loss : 0.8153864741325378\n",
      "Epoch: 3/300, train loss : 0.6752772778272629, validation loss : 0.6707612872123718\n",
      "Epoch: 4/300, train loss : 0.6795459985733032, validation loss : 0.6689957976341248\n",
      "Epoch: 5/300, train loss : 0.6380354315042496, validation loss : 0.723686933517456\n",
      "Epoch: 6/300, train loss : 0.6218600869178772, validation loss : 0.6928249001502991\n",
      "Epoch: 7/300, train loss : 0.6037453413009644, validation loss : 0.6510026454925537\n",
      "Epoch: 8/300, train loss : 0.5947704911231995, validation loss : 0.6561667919158936\n",
      "Epoch: 9/300, train loss : 0.5865524858236313, validation loss : 0.676849901676178\n",
      "Epoch: 10/300, train loss : 0.5790757536888123, validation loss : 0.6802766919136047\n",
      "Epoch: 11/300, train loss : 0.5753744393587112, validation loss : 0.651157021522522\n",
      "Epoch: 12/300, train loss : 0.5615348219871521, validation loss : 0.6443037986755371\n",
      "Epoch: 13/300, train loss : 0.5540667772293091, validation loss : 0.6599427461624146\n",
      "Epoch: 14/300, train loss : 0.5501779168844223, validation loss : 0.6645656824111938\n",
      "Epoch: 15/300, train loss : 0.5372970998287201, validation loss : 0.6496115922927856\n",
      "Epoch: 16/300, train loss : 0.5415744930505753, validation loss : 0.669983446598053\n",
      "Epoch: 17/300, train loss : 0.5245441049337387, validation loss : 0.6727775931358337\n",
      "Epoch: 18/300, train loss : 0.5183250978589058, validation loss : 0.6621517539024353\n",
      "Epoch: 19/300, train loss : 0.5106813907623291, validation loss : 0.6707349419593811\n",
      "Epoch: 20/300, train loss : 0.5021907463669777, validation loss : 0.6745347380638123\n",
      "Epoch: 21/300, train loss : 0.509017288684845, validation loss : 0.6683522462844849\n",
      "Epoch: 22/300, train loss : 0.5014842301607132, validation loss : 0.6768476366996765\n",
      "Epoch: 23/300, train loss : 0.4844155088067055, validation loss : 0.6962189078330994\n",
      "Epoch: 24/300, train loss : 0.4842185527086258, validation loss : 0.6807613372802734\n",
      "Epoch: 25/300, train loss : 0.4749724417924881, validation loss : 0.6873053908348083\n",
      "Epoch: 26/300, train loss : 0.4650858864188194, validation loss : 0.7084623575210571\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.45823387056589127, validation loss : 0.6993318200111389\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.4482584521174431, validation loss : 0.7140873074531555\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.4329385980963707, validation loss : 0.7196674942970276\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.42404475808143616, validation loss : 0.7281516790390015\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.4183342158794403, validation loss : 0.7213691473007202\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.41824715584516525, validation loss : 0.7272159457206726\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.40191660821437836, validation loss : 0.7773297429084778\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4129423424601555, validation loss : 0.7590906620025635\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4046493098139763, validation loss : 0.7866835594177246\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.3807592913508415, validation loss : 0.7617388367652893\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.3700288087129593, validation loss : 0.7983043193817139\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.37551215291023254, validation loss : 0.7604818344116211\n",
      "Early stopping...\n",
      "bce:0.6993318200111389, acc :0.5930735930735931, f1: 0.47191011235955055, roc_auc: 0.6393849976704457\n",
      "bce:0.42753754556179047, acc :0.8122702205882353, f1: 0.8438071995118974, roc_auc: 0.8687158999658999\n",
      "Rep no 3, Fold no 4\n",
      "Epoch: 1/300, train loss : 1.4181728214025497, validation loss : 0.7921507954597473\n",
      "Epoch: 2/300, train loss : 0.7452310621738434, validation loss : 0.7313492894172668\n",
      "Epoch: 3/300, train loss : 0.6881860643625259, validation loss : 0.6833483576774597\n",
      "Epoch: 4/300, train loss : 0.6831757128238678, validation loss : 0.6777384877204895\n",
      "Epoch: 5/300, train loss : 0.6706948727369308, validation loss : 0.6854179501533508\n",
      "Epoch: 6/300, train loss : 0.6394777446985245, validation loss : 0.6372374296188354\n",
      "Epoch: 7/300, train loss : 0.6235814988613129, validation loss : 0.632280170917511\n",
      "Epoch: 8/300, train loss : 0.6165425181388855, validation loss : 0.6307787299156189\n",
      "Epoch: 9/300, train loss : 0.6128071695566177, validation loss : 0.6273016333580017\n",
      "Epoch: 10/300, train loss : 0.6024020612239838, validation loss : 0.6283718347549438\n",
      "Epoch: 11/300, train loss : 0.5982425510883331, validation loss : 0.62558913230896\n",
      "Epoch: 12/300, train loss : 0.5868452489376068, validation loss : 0.6253333687782288\n",
      "Epoch: 13/300, train loss : 0.5752451717853546, validation loss : 0.6338682770729065\n",
      "Epoch: 14/300, train loss : 0.5798025578260422, validation loss : 0.6371756792068481\n",
      "Epoch: 15/300, train loss : 0.5687634199857712, validation loss : 0.6389849185943604\n",
      "Epoch: 16/300, train loss : 0.5633808523416519, validation loss : 0.6606652140617371\n",
      "Epoch: 17/300, train loss : 0.5547727197408676, validation loss : 0.65111243724823\n",
      "Epoch: 18/300, train loss : 0.5505975931882858, validation loss : 0.6568947434425354\n",
      "Epoch: 19/300, train loss : 0.5427879989147186, validation loss : 0.6638454794883728\n",
      "Epoch: 20/300, train loss : 0.5328952670097351, validation loss : 0.6659713387489319\n",
      "Epoch: 21/300, train loss : 0.5312919765710831, validation loss : 0.6734161376953125\n",
      "Epoch: 22/300, train loss : 0.5289053097367287, validation loss : 0.6841787099838257\n",
      "Epoch: 23/300, train loss : 0.5053860992193222, validation loss : 0.6974073648452759\n",
      "Epoch: 24/300, train loss : 0.5064924135804176, validation loss : 0.6902603507041931\n",
      "Epoch: 25/300, train loss : 0.5038355812430382, validation loss : 0.7236356139183044\n",
      "Epoch: 26/300, train loss : 0.4917000085115433, validation loss : 0.7419493198394775\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.49285927414894104, validation loss : 0.7513678669929504\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.48244232684373856, validation loss : 0.7338554263114929\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.4584612399339676, validation loss : 0.7770196795463562\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4658643454313278, validation loss : 0.7757457494735718\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.4480113387107849, validation loss : 0.7891319394111633\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.448064960539341, validation loss : 0.7840496897697449\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.4301956817507744, validation loss : 0.8478879928588867\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.4355674758553505, validation loss : 0.8409360647201538\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.42560169845819473, validation loss : 0.816619336605072\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.40540624409914017, validation loss : 0.8718346357345581\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.40378977358341217, validation loss : 0.8811582922935486\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.3851811811327934, validation loss : 0.9275774359703064\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.38878384232521057, validation loss : 0.9371070861816406\n",
      "Early stopping...\n",
      "bce:0.7338552474975586, acc :0.6883116883116883, f1: 0.6974789915966385, roc_auc: 0.7350600600600601\n",
      "bce:0.46491390466690063, acc :0.7887178308823529, f1: 0.8307203389830509, roc_auc: 0.8531474468974469\n",
      "Rep no 4, Fold no 0\n",
      "Epoch: 1/300, train loss : 1.3573159724473953, validation loss : 0.6672883629798889\n",
      "Epoch: 2/300, train loss : 0.7439104169607162, validation loss : 0.6624475717544556\n",
      "Epoch: 3/300, train loss : 0.7037111967802048, validation loss : 0.7322510480880737\n",
      "Epoch: 4/300, train loss : 0.6863258481025696, validation loss : 0.6494007706642151\n",
      "Epoch: 5/300, train loss : 0.6629909127950668, validation loss : 0.6637245416641235\n",
      "Epoch: 6/300, train loss : 0.6509626507759094, validation loss : 0.649994969367981\n",
      "Epoch: 7/300, train loss : 0.6347455382347107, validation loss : 0.6042147874832153\n",
      "Epoch: 8/300, train loss : 0.6238794177770615, validation loss : 0.6015206575393677\n",
      "Epoch: 9/300, train loss : 0.6110856831073761, validation loss : 0.6024836897850037\n",
      "Epoch: 10/300, train loss : 0.6115500032901764, validation loss : 0.5988104939460754\n",
      "Epoch: 11/300, train loss : 0.5965140759944916, validation loss : 0.6063761711120605\n",
      "Epoch: 12/300, train loss : 0.5987613201141357, validation loss : 0.6027013659477234\n",
      "Epoch: 13/300, train loss : 0.5870594084262848, validation loss : 0.5996884107589722\n",
      "Epoch: 14/300, train loss : 0.5851656645536423, validation loss : 0.5952556729316711\n",
      "Epoch: 15/300, train loss : 0.58275406062603, validation loss : 0.5936522483825684\n",
      "Epoch: 16/300, train loss : 0.5743226855993271, validation loss : 0.6014728546142578\n",
      "Epoch: 17/300, train loss : 0.5626812726259232, validation loss : 0.5910230278968811\n",
      "Epoch: 18/300, train loss : 0.5541758686304092, validation loss : 0.6008211374282837\n",
      "Epoch: 19/300, train loss : 0.54462730884552, validation loss : 0.578381359577179\n",
      "Epoch: 20/300, train loss : 0.5359535068273544, validation loss : 0.5958539247512817\n",
      "Epoch: 21/300, train loss : 0.5399874746799469, validation loss : 0.5863900780677795\n",
      "Epoch: 22/300, train loss : 0.5514761209487915, validation loss : 0.5873866081237793\n",
      "Epoch: 23/300, train loss : 0.5445461422204971, validation loss : 0.6401464343070984\n",
      "Epoch: 24/300, train loss : 0.5253819227218628, validation loss : 0.5863590836524963\n",
      "Epoch: 25/300, train loss : 0.5165097564458847, validation loss : 0.6347681283950806\n",
      "Epoch: 26/300, train loss : 0.5037370771169662, validation loss : 0.5921949148178101\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.5025112181901932, validation loss : 0.5908867120742798\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.49965159595012665, validation loss : 0.6591457724571228\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.4953385517001152, validation loss : 0.606550395488739\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.47631117701530457, validation loss : 0.6394698023796082\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.47157764434814453, validation loss : 0.6323249340057373\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.46956274658441544, validation loss : 0.6310318112373352\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.45927558839321136, validation loss : 0.6309683322906494\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.4532722905278206, validation loss : 0.6655375957489014\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4573613107204437, validation loss : 0.6608210802078247\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.43940619379282, validation loss : 0.6483122110366821\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.4228511154651642, validation loss : 0.6546384692192078\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.424166738986969, validation loss : 0.6596459746360779\n",
      "Early stopping...\n",
      "bce:0.5908867120742798, acc :0.6982758620689655, f1: 0.7852760736196318, roc_auc: 0.7074055330634278\n",
      "bce:0.4272451251745224, acc :0.8083639705882353, f1: 0.8446796338672768, roc_auc: 0.87862415987416\n",
      "Rep no 4, Fold no 1\n",
      "Epoch: 1/300, train loss : 1.276012808084488, validation loss : 0.678240954875946\n",
      "Epoch: 2/300, train loss : 0.7270749807357788, validation loss : 0.6882054805755615\n",
      "Epoch: 3/300, train loss : 0.6953629404306412, validation loss : 0.7465770840644836\n",
      "Epoch: 4/300, train loss : 0.6709262281656265, validation loss : 0.65785151720047\n",
      "Epoch: 5/300, train loss : 0.6572582274675369, validation loss : 0.6460785865783691\n",
      "Epoch: 6/300, train loss : 0.6289328932762146, validation loss : 0.6490728259086609\n",
      "Epoch: 7/300, train loss : 0.6158453822135925, validation loss : 0.6283251047134399\n",
      "Epoch: 8/300, train loss : 0.6110800802707672, validation loss : 0.6171633005142212\n",
      "Epoch: 9/300, train loss : 0.5970676690340042, validation loss : 0.6245472431182861\n",
      "Epoch: 10/300, train loss : 0.5932972282171249, validation loss : 0.6276569962501526\n",
      "Epoch: 11/300, train loss : 0.5799682140350342, validation loss : 0.6220129132270813\n",
      "Epoch: 12/300, train loss : 0.5759606212377548, validation loss : 0.6234132647514343\n",
      "Epoch: 13/300, train loss : 0.566747322678566, validation loss : 0.6276793479919434\n",
      "Epoch: 14/300, train loss : 0.562211811542511, validation loss : 0.6236754059791565\n",
      "Epoch: 15/300, train loss : 0.5567542910575867, validation loss : 0.6298614740371704\n",
      "Epoch: 16/300, train loss : 0.5512861609458923, validation loss : 0.6346017122268677\n",
      "Epoch: 17/300, train loss : 0.53889299929142, validation loss : 0.6265068054199219\n",
      "Epoch: 18/300, train loss : 0.5336164236068726, validation loss : 0.6368795037269592\n",
      "Epoch: 19/300, train loss : 0.5189941823482513, validation loss : 0.636258065700531\n",
      "Epoch: 20/300, train loss : 0.5056863203644753, validation loss : 0.6595804691314697\n",
      "Epoch: 21/300, train loss : 0.5158459842205048, validation loss : 0.6520137786865234\n",
      "Epoch: 22/300, train loss : 0.5130764245986938, validation loss : 0.6556864976882935\n",
      "Epoch: 23/300, train loss : 0.5102577805519104, validation loss : 0.6558301448822021\n",
      "Epoch: 24/300, train loss : 0.49531102925539017, validation loss : 0.658711314201355\n",
      "Epoch: 25/300, train loss : 0.4801264852285385, validation loss : 0.6925159096717834\n",
      "Epoch: 26/300, train loss : 0.4738806113600731, validation loss : 0.6716874837875366\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.46419263631105423, validation loss : 0.6925167441368103\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4567567706108093, validation loss : 0.6892433166503906\n",
      "Early stop counter: 2\n",
      "Epoch: 29/300, train loss : 0.44943082332611084, validation loss : 0.7053476572036743\n",
      "Early stop counter: 3\n",
      "Epoch: 30/300, train loss : 0.4354974403977394, validation loss : 0.7126944661140442\n",
      "Early stop counter: 4\n",
      "Epoch: 31/300, train loss : 0.43234269320964813, validation loss : 0.7037266492843628\n",
      "Early stop counter: 5\n",
      "Epoch: 32/300, train loss : 0.42717012763023376, validation loss : 0.7259414792060852\n",
      "Early stop counter: 6\n",
      "Epoch: 33/300, train loss : 0.42544714361429214, validation loss : 0.7365689873695374\n",
      "Early stop counter: 7\n",
      "Epoch: 34/300, train loss : 0.40308839827775955, validation loss : 0.7234514951705933\n",
      "Early stop counter: 8\n",
      "Epoch: 35/300, train loss : 0.40092699229717255, validation loss : 0.7508580684661865\n",
      "Early stop counter: 9\n",
      "Epoch: 36/300, train loss : 0.39847055077552795, validation loss : 0.7839818596839905\n",
      "Early stop counter: 10\n",
      "Epoch: 37/300, train loss : 0.37568680197000504, validation loss : 0.7813761234283447\n",
      "Early stopping...\n",
      "bce:0.6716874837875366, acc :0.646551724137931, f1: 0.7421383647798742, roc_auc: 0.6238495398159264\n",
      "bce:0.44908852875232697, acc :0.8308823529411764, f1: 0.8647558386411889, roc_auc: 0.8470286907786908\n",
      "Rep no 4, Fold no 2\n",
      "Epoch: 1/300, train loss : 1.3414556831121445, validation loss : 0.761675238609314\n",
      "Epoch: 2/300, train loss : 0.717856764793396, validation loss : 0.7416423559188843\n",
      "Epoch: 3/300, train loss : 0.6655282378196716, validation loss : 0.6929344534873962\n",
      "Epoch: 4/300, train loss : 0.6612602770328522, validation loss : 0.6823170185089111\n",
      "Epoch: 5/300, train loss : 0.6455835402011871, validation loss : 0.694117546081543\n",
      "Epoch: 6/300, train loss : 0.6189844012260437, validation loss : 0.6628841757774353\n",
      "Epoch: 7/300, train loss : 0.6030892580747604, validation loss : 0.6636627912521362\n",
      "Epoch: 8/300, train loss : 0.5998332798480988, validation loss : 0.6703013181686401\n",
      "Epoch: 9/300, train loss : 0.5870531052350998, validation loss : 0.6658790111541748\n",
      "Epoch: 10/300, train loss : 0.58246248960495, validation loss : 0.6614856123924255\n",
      "Epoch: 11/300, train loss : 0.5746673345565796, validation loss : 0.6595599055290222\n",
      "Epoch: 12/300, train loss : 0.5620504170656204, validation loss : 0.6633220314979553\n",
      "Epoch: 13/300, train loss : 0.5597740113735199, validation loss : 0.6730812191963196\n",
      "Epoch: 14/300, train loss : 0.549988329410553, validation loss : 0.6754938960075378\n",
      "Epoch: 15/300, train loss : 0.548926830291748, validation loss : 0.6682283878326416\n",
      "Epoch: 16/300, train loss : 0.5387837141752243, validation loss : 0.6671674251556396\n",
      "Epoch: 17/300, train loss : 0.5380290895700455, validation loss : 0.6709808111190796\n",
      "Epoch: 18/300, train loss : 0.5256337597966194, validation loss : 0.6610444188117981\n",
      "Epoch: 19/300, train loss : 0.5215007960796356, validation loss : 0.6611846685409546\n",
      "Epoch: 20/300, train loss : 0.5072024464607239, validation loss : 0.6727803945541382\n",
      "Epoch: 21/300, train loss : 0.5156775563955307, validation loss : 0.6840396523475647\n",
      "Epoch: 22/300, train loss : 0.504919707775116, validation loss : 0.6827175617218018\n",
      "Epoch: 23/300, train loss : 0.4979143813252449, validation loss : 0.6783404350280762\n",
      "Epoch: 24/300, train loss : 0.48715975135564804, validation loss : 0.6725091338157654\n",
      "Epoch: 25/300, train loss : 0.47493206709623337, validation loss : 0.6789994835853577\n",
      "Epoch: 26/300, train loss : 0.46948400884866714, validation loss : 0.6996865272521973\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.4648382142186165, validation loss : 0.6967502236366272\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44879064708948135, validation loss : 0.6903845071792603\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.44420329481363297, validation loss : 0.69973224401474\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.4393465742468834, validation loss : 0.7174944877624512\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.41999179869890213, validation loss : 0.7140414118766785\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.41930337250232697, validation loss : 0.7488390803337097\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.4060996547341347, validation loss : 0.7257780432701111\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.39847835153341293, validation loss : 0.7523772120475769\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.3988659828901291, validation loss : 0.7484878301620483\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.3703993856906891, validation loss : 0.7966592907905579\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.3688502758741379, validation loss : 0.7743175029754639\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.398568294942379, validation loss : 0.8088264465332031\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.36350836604833603, validation loss : 0.7836071252822876\n",
      "Early stopping...\n",
      "bce:0.6903845071792603, acc :0.5930735930735931, f1: 0.5948275862068966, roc_auc: 0.6429428528573572\n",
      "bce:0.4850013107061386, acc :0.7750459558823529, f1: 0.8267579173376275, roc_auc: 0.8370688683188684\n",
      "Rep no 4, Fold no 3\n",
      "Epoch: 1/300, train loss : 1.319130778312683, validation loss : 0.8232491612434387\n",
      "Epoch: 2/300, train loss : 0.7354132384061813, validation loss : 0.8153864741325378\n",
      "Epoch: 3/300, train loss : 0.6752773076295853, validation loss : 0.6707612872123718\n",
      "Epoch: 4/300, train loss : 0.679545983672142, validation loss : 0.6689958572387695\n",
      "Epoch: 5/300, train loss : 0.6380354315042496, validation loss : 0.7236869931221008\n",
      "Epoch: 6/300, train loss : 0.6218601018190384, validation loss : 0.6928249597549438\n",
      "Epoch: 7/300, train loss : 0.6037453413009644, validation loss : 0.6510026454925537\n",
      "Epoch: 8/300, train loss : 0.5947704911231995, validation loss : 0.6561667323112488\n",
      "Epoch: 9/300, train loss : 0.5865524858236313, validation loss : 0.676849901676178\n",
      "Epoch: 10/300, train loss : 0.5790757387876511, validation loss : 0.6802766919136047\n",
      "Epoch: 11/300, train loss : 0.5753743797540665, validation loss : 0.6511572599411011\n",
      "Epoch: 12/300, train loss : 0.5615351647138596, validation loss : 0.6443095803260803\n",
      "Epoch: 13/300, train loss : 0.5540633499622345, validation loss : 0.6599515676498413\n",
      "Epoch: 14/300, train loss : 0.5501698404550552, validation loss : 0.6645416617393494\n",
      "Epoch: 15/300, train loss : 0.5372805520892143, validation loss : 0.649577796459198\n",
      "Epoch: 16/300, train loss : 0.5416046530008316, validation loss : 0.6699749827384949\n",
      "Epoch: 17/300, train loss : 0.5245926678180695, validation loss : 0.6730800867080688\n",
      "Epoch: 18/300, train loss : 0.518331915140152, validation loss : 0.6625931859016418\n",
      "Epoch: 19/300, train loss : 0.5107820928096771, validation loss : 0.6710361242294312\n",
      "Epoch: 20/300, train loss : 0.5021838024258614, validation loss : 0.6749915480613708\n",
      "Epoch: 21/300, train loss : 0.5091070607304573, validation loss : 0.6685143709182739\n",
      "Epoch: 22/300, train loss : 0.501428447663784, validation loss : 0.6773073673248291\n",
      "Epoch: 23/300, train loss : 0.48433615267276764, validation loss : 0.6959612369537354\n",
      "Epoch: 24/300, train loss : 0.48405420780181885, validation loss : 0.6813366413116455\n",
      "Epoch: 25/300, train loss : 0.47477103769779205, validation loss : 0.6862485408782959\n",
      "Epoch: 26/300, train loss : 0.46471667289733887, validation loss : 0.7106797099113464\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.45818084478378296, validation loss : 0.6988873481750488\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 28/300, train loss : 0.44744332879781723, validation loss : 0.717496931552887\n",
      "Early stop counter: 1\n",
      "Epoch: 29/300, train loss : 0.4340202286839485, validation loss : 0.7179051637649536\n",
      "Early stop counter: 2\n",
      "Epoch: 30/300, train loss : 0.4232368618249893, validation loss : 0.7326188683509827\n",
      "Early stop counter: 3\n",
      "Epoch: 31/300, train loss : 0.41925401985645294, validation loss : 0.721926748752594\n",
      "Early stop counter: 4\n",
      "Epoch: 32/300, train loss : 0.4183647781610489, validation loss : 0.7290934920310974\n",
      "Early stop counter: 5\n",
      "Epoch: 33/300, train loss : 0.40159230679273605, validation loss : 0.7821969985961914\n",
      "Early stop counter: 6\n",
      "Epoch: 34/300, train loss : 0.413538821041584, validation loss : 0.7556315064430237\n",
      "Early stop counter: 7\n",
      "Epoch: 35/300, train loss : 0.4055883064866066, validation loss : 0.7781493663787842\n",
      "Early stop counter: 8\n",
      "Epoch: 36/300, train loss : 0.380135253071785, validation loss : 0.761509358882904\n",
      "Early stop counter: 9\n",
      "Epoch: 37/300, train loss : 0.37060605734586716, validation loss : 0.8015485405921936\n",
      "Early stop counter: 10\n",
      "Epoch: 38/300, train loss : 0.37350279837846756, validation loss : 0.7647857069969177\n",
      "Early stopping...\n",
      "bce:0.698887288570404, acc :0.5887445887445888, f1: 0.46327683615819215, roc_auc: 0.6393073458611584\n",
      "bce:0.4283812344074249, acc :0.8103170955882353, f1: 0.8417508417508417, roc_auc: 0.8679651492151492\n",
      "Rep no 4, Fold no 4\n",
      "Epoch: 1/300, train loss : 1.4181728214025497, validation loss : 0.7921507358551025\n",
      "Epoch: 2/300, train loss : 0.7452310472726822, validation loss : 0.7313492894172668\n",
      "Epoch: 3/300, train loss : 0.6881860494613647, validation loss : 0.6833483576774597\n",
      "Epoch: 4/300, train loss : 0.683175727725029, validation loss : 0.6777384877204895\n",
      "Epoch: 5/300, train loss : 0.6706948578357697, validation loss : 0.6854179501533508\n",
      "Epoch: 6/300, train loss : 0.6394777595996857, validation loss : 0.6372373104095459\n",
      "Epoch: 7/300, train loss : 0.6235810071229935, validation loss : 0.6322788000106812\n",
      "Epoch: 8/300, train loss : 0.6165443360805511, validation loss : 0.6307584643363953\n",
      "Epoch: 9/300, train loss : 0.6128071546554565, validation loss : 0.6272329092025757\n",
      "Epoch: 10/300, train loss : 0.6024027019739151, validation loss : 0.628291666507721\n",
      "Epoch: 11/300, train loss : 0.5982253700494766, validation loss : 0.6256172060966492\n",
      "Epoch: 12/300, train loss : 0.5868613719940186, validation loss : 0.6253120303153992\n",
      "Epoch: 13/300, train loss : 0.5753334760665894, validation loss : 0.6337693929672241\n",
      "Epoch: 14/300, train loss : 0.5797493010759354, validation loss : 0.6370149850845337\n",
      "Epoch: 15/300, train loss : 0.568738117814064, validation loss : 0.6386848092079163\n",
      "Epoch: 16/300, train loss : 0.5634787231683731, validation loss : 0.6614140868186951\n",
      "Epoch: 17/300, train loss : 0.5548924803733826, validation loss : 0.6512377262115479\n",
      "Epoch: 18/300, train loss : 0.5507174432277679, validation loss : 0.6564134359359741\n",
      "Epoch: 19/300, train loss : 0.5431306213140488, validation loss : 0.6636702418327332\n",
      "Epoch: 20/300, train loss : 0.5332628637552261, validation loss : 0.665228009223938\n",
      "Epoch: 21/300, train loss : 0.5313248038291931, validation loss : 0.6746136546134949\n",
      "Epoch: 22/300, train loss : 0.5283108651638031, validation loss : 0.6857751607894897\n",
      "Epoch: 23/300, train loss : 0.505538210272789, validation loss : 0.6940894722938538\n",
      "Epoch: 24/300, train loss : 0.5063661560416222, validation loss : 0.6885900497436523\n",
      "Epoch: 25/300, train loss : 0.5037974342703819, validation loss : 0.7213496565818787\n",
      "Epoch: 26/300, train loss : 0.49326711893081665, validation loss : 0.7414050698280334\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 27/300, train loss : 0.49401771277189255, validation loss : 0.7506381869316101\n",
      "Early stop counter: 1\n",
      "Epoch: 28/300, train loss : 0.4831355959177017, validation loss : 0.72740638256073\n",
      "Saving model...\n",
      "Early stop counter: 0\n",
      "Epoch: 29/300, train loss : 0.45901157706975937, validation loss : 0.7756253480911255\n",
      "Early stop counter: 1\n",
      "Epoch: 30/300, train loss : 0.46566614508628845, validation loss : 0.7781760692596436\n",
      "Early stop counter: 2\n",
      "Epoch: 31/300, train loss : 0.44902148842811584, validation loss : 0.7908423542976379\n",
      "Early stop counter: 3\n",
      "Epoch: 32/300, train loss : 0.44717954844236374, validation loss : 0.7853613495826721\n",
      "Early stop counter: 4\n",
      "Epoch: 33/300, train loss : 0.43123120814561844, validation loss : 0.848675012588501\n",
      "Early stop counter: 5\n",
      "Epoch: 34/300, train loss : 0.43491991609334946, validation loss : 0.8368276357650757\n",
      "Early stop counter: 6\n",
      "Epoch: 35/300, train loss : 0.4254450127482414, validation loss : 0.8190051317214966\n",
      "Early stop counter: 7\n",
      "Epoch: 36/300, train loss : 0.40356986969709396, validation loss : 0.8681742548942566\n",
      "Early stop counter: 8\n",
      "Epoch: 37/300, train loss : 0.4037903770804405, validation loss : 0.8775939345359802\n",
      "Early stop counter: 9\n",
      "Epoch: 38/300, train loss : 0.38457054644823074, validation loss : 0.926056444644928\n",
      "Early stop counter: 10\n",
      "Epoch: 39/300, train loss : 0.38835667073726654, validation loss : 0.9379691481590271\n",
      "Early stopping...\n",
      "bce:0.72740638256073, acc :0.696969696969697, f1: 0.7033898305084746, roc_auc: 0.734984984984985\n",
      "bce:0.4649890512228012, acc :0.7867647058823529, f1: 0.8286564625850341, roc_auc: 0.8512221012221012\n",
      "validation bce:0.677±0.047\n",
      "validation acc:0.646±0.045\n",
      "validation f1: 0.658±0.113\n",
      "validation roc_auc: 0.669±0.044\n",
      "bce:0.450±0.021\n",
      "acc:0.804±0.016\n",
      "f1: 0.843±0.012\n",
      "roc_auc: 0.856±0.015\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 25\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_5x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 30\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tune 10x, force training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5746744275093079, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.722165991902834\n",
      "bce:0.47143182158470154, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8689551502051502\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6113265156745911, acc :0.6982758620689655, f1: 0.7852760736196319, roc_auc: 0.675390156062425\n",
      "bce:0.4705562889575958, acc :0.7897518382352942, f1: 0.8315412186379929, roc_auc: 0.8604999229999231\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6408312916755676, acc :0.6017316017316018, f1: 0.6166666666666667, roc_auc: 0.6661166941652918\n",
      "bce:0.4608095586299896, acc :0.8005514705882353, f1: 0.8388746803069054, roc_auc: 0.8693649006149007\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.664361834526062, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.48236191272735596, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.85879664004664\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6160662770271301, acc :0.7056277056277056, f1: 0.6991150442477877, roc_auc: 0.7243243243243244\n",
      "bce:0.5049171596765518, acc :0.7750459558823529, f1: 0.8203532136653156, roc_auc: 0.8419236544236545\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.574566662311554, acc :0.6767241379310345, f1: 0.7734138972809668, roc_auc: 0.722165991902834\n",
      "bce:0.47122280299663544, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8684546497046497\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6114678382873535, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6773909563825529\n",
      "bce:0.4746283143758774, acc :0.7877987132352942, f1: 0.8302965344765989, roc_auc: 0.8651326463826465\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6407890915870667, acc :0.6017316017316018, f1: 0.6166666666666667, roc_auc: 0.6659667016649168\n",
      "bce:0.4608020931482315, acc :0.7985983455882353, f1: 0.8376292309871124, roc_auc: 0.8696777134277134\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6644201278686523, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.4822881817817688, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8586402336402337\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6161682605743408, acc :0.7056277056277056, f1: 0.6991150442477877, roc_auc: 0.7244744744744744\n",
      "bce:0.5048010051250458, acc :0.7750459558823529, f1: 0.8203532136653156, roc_auc: 0.8418923731423732\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5753085017204285, acc :0.6724137931034483, f1: 0.7696969696969699, roc_auc: 0.7208164642375169\n",
      "bce:0.47124598920345306, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8682043994543995\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6113358736038208, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6779511804721888\n",
      "bce:0.4743354767560959, acc :0.7858455882352942, f1: 0.829794762915782, roc_auc: 0.8652890527890529\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6406758427619934, acc :0.5887445887445888, f1: 0.5777777777777777, roc_auc: 0.6658167091645418\n",
      "bce:0.4581283628940582, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8718533093533094\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.66439288854599, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6108091318527723\n",
      "bce:0.482328325510025, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8587653587653588\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6161780953407288, acc :0.7056277056277056, f1: 0.6991150442477877, roc_auc: 0.7245495495495495\n",
      "bce:0.5049134641885757, acc :0.7730928308823529, f1: 0.8191197691197691, roc_auc: 0.8417985292985293\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5755044221878052, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7202260458839407\n",
      "bce:0.4711360037326813, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8687048999548999\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6114908456802368, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.678031212484994\n",
      "bce:0.4745740592479706, acc :0.7858455882352942, f1: 0.8290598290598292, roc_auc: 0.865101365101365\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6407508254051208, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6662666866656667\n",
      "bce:0.46108514070510864, acc :0.7966452205882353, f1: 0.8363918690005647, roc_auc: 0.869302338052338\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6643617749214172, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.48236191272735596, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.85879664004664\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6161802411079407, acc :0.7056277056277056, f1: 0.6991150442477877, roc_auc: 0.7245495495495495\n",
      "bce:0.5049129873514175, acc :0.7730928308823529, f1: 0.8191197691197691, roc_auc: 0.841767248017248\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5760742425918579, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7194669365721997\n",
      "bce:0.4706093966960907, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8686736186736186\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6114146709442139, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6783513405362144\n",
      "bce:0.4745423197746277, acc :0.7877987132352942, f1: 0.8302965344765989, roc_auc: 0.8655705843205843\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6407461166381836, acc :0.6017316017316018, f1: 0.6166666666666667, roc_auc: 0.6665666716664167\n",
      "bce:0.4616140127182007, acc :0.7946920955882353, f1: 0.835162515829464, roc_auc: 0.869177212927213\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6644144654273987, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6106538282341979\n",
      "bce:0.48229050636291504, acc :0.7780330882352942, f1: 0.8249475890985325, roc_auc: 0.8586402336402337\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6161632537841797, acc :0.7056277056277056, f1: 0.6991150442477877, roc_auc: 0.7244744744744744\n",
      "bce:0.5047960579395294, acc :0.7750459558823529, f1: 0.8203532136653156, roc_auc: 0.8419549357049357\n",
      "validation bce:0.622±0.030\n",
      "validation acc:0.656±0.050\n",
      "validation f1: 0.675±0.108\n",
      "validation roc_auc: 0.680±0.042\n",
      "bce:0.479±0.015\n",
      "acc:0.788±0.011\n",
      "f1: 0.831±0.009\n",
      "roc_auc: 0.861±0.010\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_10x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 10\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.5755579471588135, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.7209851551956814\n",
      "bce:0.4701726883649826, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8687987437987438\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6114091277122498, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.67859143657463\n",
      "bce:0.474857896566391, acc :0.7897518382352942, f1: 0.8322649572649572, roc_auc: 0.8656644281644281\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6407521963119507, acc :0.5887445887445888, f1: 0.5777777777777777, roc_auc: 0.6650667466626669\n",
      "bce:0.4570174068212509, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8720722783222783\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6759536266326904, acc :0.5800865800865801, f1: 0.5076142131979696, roc_auc: 0.6108867836620593\n",
      "bce:0.46198470890522003, acc :0.8044577205882353, f1: 0.8491597759402507, roc_auc: 0.8671549296549297\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.6232619285583496, acc :0.7056277056277056, f1: 0.706896551724138, roc_auc: 0.7322822822822823\n",
      "bce:0.4692782908678055, acc :0.7926240808823529, f1: 0.8381514657980456, roc_auc: 0.8669125856625857\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5765438675880432, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7222503373819164\n",
      "bce:0.47027307748794556, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8686423373923373\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6109812259674072, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6785114045618247\n",
      "bce:0.4739714413881302, acc :0.7897518382352942, f1: 0.8322649572649572, roc_auc: 0.8656018656018656\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6409894227981567, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6657417129143542\n",
      "bce:0.4608929455280304, acc :0.7966452205882353, f1: 0.8363918690005647, roc_auc: 0.8695525883025883\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6758866906166077, acc :0.5800865800865801, f1: 0.5076142131979696, roc_auc: 0.6111197390899208\n",
      "bce:0.4619671255350113, acc :0.8044577205882353, f1: 0.8491597759402507, roc_auc: 0.8670923670923671\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6229758262634277, acc :0.70995670995671, f1: 0.70995670995671, roc_auc: 0.7305555555555555\n",
      "bce:0.4729216396808624, acc :0.7926240808823529, f1: 0.8381514657980456, roc_auc: 0.8598852973852974\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5752788782119751, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7212381916329285\n",
      "bce:0.4705493897199631, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.868830025080025\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6110859513282776, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6787515006002401\n",
      "bce:0.4748232811689377, acc :0.7897518382352942, f1: 0.8322649572649572, roc_auc: 0.8655705843205844\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6407975554466248, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6663416829158542\n",
      "bce:0.46128641068935394, acc :0.7985983455882353, f1: 0.8376292309871124, roc_auc: 0.8674395549395549\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6759827136993408, acc :0.5800865800865801, f1: 0.5076142131979696, roc_auc: 0.6109644354713464\n",
      "bce:0.4621787518262863, acc :0.8044577205882353, f1: 0.8491597759402507, roc_auc: 0.8690802753302753\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6232154965400696, acc :0.70995670995671, f1: 0.70995670995671, roc_auc: 0.7323573573573572\n",
      "bce:0.46933652460575104, acc :0.7926240808823529, f1: 0.8381514657980456, roc_auc: 0.8670064295064295\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5749793648719788, acc :0.6853448275862069, f1: 0.7794561933534743, roc_auc: 0.7214912280701754\n",
      "bce:0.47018903493881226, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8687987437987438\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6116852760314941, acc :0.6982758620689655, f1: 0.7852760736196319, roc_auc: 0.6735494197679072\n",
      "bce:0.4712010324001312, acc :0.7897518382352942, f1: 0.8315412186379929, roc_auc: 0.8590750778250779\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6406441330909729, acc :0.6017316017316018, f1: 0.6166666666666667, roc_auc: 0.6664916754162291\n",
      "bce:0.46113796532154083, acc :0.7985983455882353, f1: 0.8376292309871124, roc_auc: 0.8674082736582737\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.674492597579956, acc :0.5930735930735931, f1: 0.5, roc_auc: 0.6076254076719988\n",
      "bce:0.45343801379203796, acc :0.8005514705882353, f1: 0.8418720576017724, roc_auc: 0.8708633083633084\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6224952936172485, acc :0.70995670995671, f1: 0.70995670995671, roc_auc: 0.7306306306306306\n",
      "bce:0.4729313254356384, acc :0.7945772058823529, f1: 0.8394607843137256, roc_auc: 0.8599165786665787\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5749348402023315, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.7212381916329285\n",
      "bce:0.47055381536483765, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8685484935484935\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6112423539161682, acc :0.7068965517241379, f1: 0.7926829268292683, roc_auc: 0.6782713085234093\n",
      "bce:0.47516877949237823, acc :0.7858455882352942, f1: 0.8290598290598292, roc_auc: 0.863801645051645\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6405910849571228, acc :0.6060606060606061, f1: 0.6224066390041494, roc_auc: 0.6666416679166042\n",
      "bce:0.4611609876155853, acc :0.7966452205882353, f1: 0.8363918690005647, roc_auc: 0.869396181896182\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.675264835357666, acc :0.5974025974025974, f1: 0.5026737967914439, roc_auc: 0.6074701040534244\n",
      "bce:0.45290185511112213, acc :0.7985983455882353, f1: 0.8399083206000835, roc_auc: 0.8728199353199353\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6233587861061096, acc :0.7056277056277056, f1: 0.706896551724138, roc_auc: 0.7322822822822822\n",
      "bce:0.4693939536809921, acc :0.7926240808823529, f1: 0.8381514657980456, roc_auc: 0.8668187418187419\n",
      "validation bce:0.625±0.033\n",
      "validation acc:0.656±0.052\n",
      "validation f1: 0.678±0.107\n",
      "validation roc_auc: 0.681±0.044\n",
      "bce:0.467±0.007\n",
      "acc:0.797±0.005\n",
      "f1: 0.840±0.006\n",
      "roc_auc: 0.867±0.003\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_10x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 15\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "        \n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep no 0, Fold no 0\n",
      "bce:0.574384868144989, acc :0.6767241379310345, f1: 0.7734138972809668, roc_auc: 0.7231781376518218\n",
      "bce:0.4712449908256531, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8684233684233684\n",
      "Rep no 0, Fold no 1\n",
      "bce:0.6162497401237488, acc :0.6810344827586207, f1: 0.7743902439024389, roc_auc: 0.6736294517807123\n",
      "bce:0.4568988233804703, acc :0.8034237132352942, f1: 0.8438054668086618, roc_auc: 0.8633949883949885\n",
      "Rep no 0, Fold no 2\n",
      "bce:0.6411864757537842, acc :0.5887445887445888, f1: 0.5777777777777777, roc_auc: 0.6649542522873856\n",
      "bce:0.45768292248249054, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8721035596035596\n",
      "Rep no 0, Fold no 3\n",
      "bce:0.6878967881202698, acc :0.5930735930735931, f1: 0.4777777777777778, roc_auc: 0.6041310762540767\n",
      "bce:0.44944070279598236, acc :0.8025045955882353, f1: 0.8431032087789971, roc_auc: 0.8653859903859904\n",
      "Rep no 0, Fold no 4\n",
      "bce:0.625647783279419, acc :0.70995670995671, f1: 0.7124463519313305, roc_auc: 0.7343093093093093\n",
      "bce:0.467437207698822, acc :0.7984834558823529, f1: 0.8445512820512822, roc_auc: 0.8642959580459582\n",
      "Rep no 1, Fold no 0\n",
      "bce:0.5754503607749939, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7215755735492577\n",
      "bce:0.47035735845565796, acc :0.7995174632352942, f1: 0.8431983385254413, roc_auc: 0.8685172122672122\n",
      "Rep no 1, Fold no 1\n",
      "bce:0.6163567900657654, acc :0.6810344827586207, f1: 0.7743902439024389, roc_auc: 0.6747498999599839\n",
      "bce:0.45771096646785736, acc :0.8014705882352942, f1: 0.8431786216596344, roc_auc: 0.8629257691757692\n",
      "Rep no 1, Fold no 2\n",
      "bce:0.6409274935722351, acc :0.5887445887445888, f1: 0.5777777777777777, roc_auc: 0.6655167241637918\n",
      "bce:0.45778997242450714, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8716656216656217\n",
      "Rep no 1, Fold no 3\n",
      "bce:0.6852952241897583, acc :0.5930735930735931, f1: 0.4835164835164835, roc_auc: 0.6065382823419786\n",
      "bce:0.4494883567094803, acc :0.8025045955882353, f1: 0.8431032087789971, roc_auc: 0.8657300844800845\n",
      "Rep no 1, Fold no 4\n",
      "bce:0.6251277327537537, acc :0.70995670995671, f1: 0.7124463519313305, roc_auc: 0.7338588588588589\n",
      "bce:0.4677822142839432, acc :0.7984834558823529, f1: 0.8445512820512822, roc_auc: 0.8640457077957078\n",
      "Rep no 2, Fold no 0\n",
      "bce:0.5758765339851379, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7202260458839407\n",
      "bce:0.47124183177948, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8685172122672122\n",
      "Rep no 2, Fold no 1\n",
      "bce:0.6165206432342529, acc :0.6810344827586207, f1: 0.7743902439024389, roc_auc: 0.6730692276910765\n",
      "bce:0.4583802670240402, acc :0.8034237132352942, f1: 0.8438054668086618, roc_auc: 0.8623001435501436\n",
      "Rep no 2, Fold no 2\n",
      "bce:0.6410956382751465, acc :0.5930735930735931, f1: 0.584070796460177, roc_auc: 0.6649917504124794\n",
      "bce:0.4574059396982193, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8719158719158719\n",
      "Rep no 2, Fold no 3\n",
      "bce:0.6856549382209778, acc :0.5930735930735931, f1: 0.4835164835164835, roc_auc: 0.6070041931977015\n",
      "bce:0.449402391910553, acc :0.8025045955882353, f1: 0.8431032087789971, roc_auc: 0.8656049593549593\n",
      "Rep no 2, Fold no 4\n",
      "bce:0.6256292462348938, acc :0.70995670995671, f1: 0.7124463519313305, roc_auc: 0.7343843843843844\n",
      "bce:0.46747781336307526, acc :0.7984834558823529, f1: 0.8445512820512822, roc_auc: 0.8643272393272394\n",
      "Rep no 3, Fold no 0\n",
      "bce:0.5746014714241028, acc :0.6810344827586207, f1: 0.7757575757575758, roc_auc: 0.7210695006747638\n",
      "bce:0.47148944437503815, acc :0.7956112132352942, f1: 0.8407292741658068, roc_auc: 0.8683920871420872\n",
      "Rep no 3, Fold no 1\n",
      "bce:0.6162528991699219, acc :0.6810344827586207, f1: 0.7743902439024389, roc_auc: 0.673469387755102\n",
      "bce:0.4569006562232971, acc :0.8034237132352942, f1: 0.8438054668086618, roc_auc: 0.8634262696762698\n",
      "Rep no 3, Fold no 2\n",
      "bce:0.6411042809486389, acc :0.5887445887445888, f1: 0.5777777777777777, roc_auc: 0.6651417429128543\n",
      "bce:0.45781783759593964, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8717281842281842\n",
      "Rep no 3, Fold no 3\n",
      "bce:0.6855892539024353, acc :0.5800865800865801, f1: 0.4699453551912568, roc_auc: 0.6059947196769685\n",
      "bce:0.449334517121315, acc :0.8005514705882353, f1: 0.8418720576017724, roc_auc: 0.8658239283239283\n",
      "Rep no 3, Fold no 4\n",
      "bce:0.6253050565719604, acc :0.70995670995671, f1: 0.7124463519313305, roc_auc: 0.7341591591591592\n",
      "bce:0.46746455132961273, acc :0.7984834558823529, f1: 0.8445512820512822, roc_auc: 0.863951863951864\n",
      "Rep no 4, Fold no 0\n",
      "bce:0.5753983855247498, acc :0.6767241379310345, f1: 0.7720364741641337, roc_auc: 0.7209851551956814\n",
      "bce:0.4707799553871155, acc :0.8014705882352942, f1: 0.8444444444444446, roc_auc: 0.8688613063613063\n",
      "Rep no 4, Fold no 1\n",
      "bce:0.6169070601463318, acc :0.6810344827586207, f1: 0.7743902439024389, roc_auc: 0.6720288115246098\n",
      "bce:0.458311066031456, acc :0.7995174632352942, f1: 0.8419207851384508, roc_auc: 0.8628319253319254\n",
      "Rep no 4, Fold no 2\n",
      "bce:0.6408268809318542, acc :0.5887445887445888, f1: 0.5814977973568283, roc_auc: 0.6653667316634169\n",
      "bce:0.45783956348896027, acc :0.8044577205882353, f1: 0.8406276993953354, roc_auc: 0.8715404965404965\n",
      "Rep no 4, Fold no 3\n",
      "bce:0.6866040229797363, acc :0.5887445887445888, f1: 0.4692737430167598, roc_auc: 0.6068488895791273\n",
      "bce:0.4496585875749588, acc :0.8064108455882353, f1: 0.8455892632461904, roc_auc: 0.8648542086042086\n",
      "Rep no 4, Fold no 4\n",
      "bce:0.6246830821037292, acc :0.7056277056277056, f1: 0.706896551724138, roc_auc: 0.7337837837837837\n",
      "bce:0.46791958808898926, acc :0.8004365808823529, f1: 0.8458601286173634, roc_auc: 0.8654392716892718\n",
      "validation bce:0.629±0.036\n",
      "validation acc:0.649±0.050\n",
      "validation f1: 0.663±0.117\n",
      "validation roc_auc: 0.680±0.046\n",
      "bce:0.461±0.008\n",
      "acc:0.802±0.003\n",
      "f1: 0.843±0.002\n",
      "roc_auc: 0.867±0.003\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "train_data_root_path = './data/graph_data/data_oral_avail_train/'\n",
    "train_data_raw_filename = 'data_oral_avail_train_50.csv'\n",
    "test_data_root_path = './data/graph_data/data_oral_avail_test'\n",
    "test_data_raw_filename = 'data_oral_avail_test_1_50.csv'\n",
    "n_repetitions = 5\n",
    "method_tf = 'fine_tune_10x'\n",
    "params = best_params_vertical\n",
    "es_trigger = 20\n",
    "path_to_pretrained_model = './trf_learning_models/pretrained_models/vertical/high/'\n",
    "path_to_save_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "path_to_trained_model = './trf_learning_models/trained_models/vertical/high/pretrained_60/'\n",
    "\n",
    "val_bce_list = []\n",
    "val_acc_list = []\n",
    "val_f1_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "bce_list = []\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "dataset_for_cv = LoadHOBDataset(train_data_root_path, train_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset = []\n",
    "        valid_dataset = []\n",
    "        \n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{t_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(\n",
    "                torch.load(\n",
    "                    f\"./data/graph_data/data_oral_avail_train/processed/molecule_{v_idx}.pt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        test_dataset = LoadHOBDataset(test_data_root_path, test_data_raw_filename)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False\n",
    "        )\n",
    "        print(f'Rep no {repeat}, Fold no {fold_no}')\n",
    "        '''\n",
    "        run_training(method_tf, train_loader, valid_loader, params, es_trigger, os.path.join(path_to_pretrained_model, f'pretrained_vertical_model_60_epoch.pt'),\n",
    "            os.path.join(\n",
    "                path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "        val_bce, val_acc, val_f1, val_roc_auc = run_validation(method_tf, valid_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "        bce, acc, f1, roc_auc = run_testing(method_tf, test_loader, params, \n",
    "                        os.path.join(path_to_save_trained_model, f\"trained_vertical_model_{method_tf}_repeat_{repeat}_fold_{fold_no}_{es_trigger}_es_trigger.pt\"))\n",
    "\n",
    "        val_bce_list.append(val_bce)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_f1_list.append(val_f1)\n",
    "        val_roc_auc_list.append(val_roc_auc)\n",
    "\n",
    "        bce_list.append(bce)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "val_bce_arr = np.array(val_bce_list)\n",
    "val_mean_bce = np.mean(val_bce_arr)\n",
    "val_sd_bce = np.std(val_bce_arr)\n",
    "print(f'validation bce:{val_mean_bce:.3f}±{val_sd_bce:.3f}')\n",
    "\n",
    "val_acc_arr = np.array(val_acc_list)\n",
    "val_acc_mean= np.mean(val_acc_arr)\n",
    "val_acc_sd = np.std(val_acc_arr)\n",
    "print(f'validation acc:{val_acc_mean:.3f}±{val_acc_sd:.3f}')\n",
    "\n",
    "val_f1_arr = np.array(val_f1_list)\n",
    "val_f1_mean= np.mean(val_f1_arr)\n",
    "val_f1_sd = np.std(val_f1_arr)\n",
    "print(f'validation f1: {val_f1_mean:.3f}±{val_f1_sd:.3f}')\n",
    "\n",
    "val_roc_auc_arr = np.array(val_roc_auc_list)\n",
    "val_roc_auc_mean= np.mean(val_roc_auc_arr)\n",
    "val_roc_auc_sd = np.std(val_roc_auc_arr)\n",
    "print(f'validation roc_auc: {val_roc_auc_mean:.3f}±{val_roc_auc_sd:.3f}')\n",
    "\n",
    "bce_arr = np.array(bce_list)\n",
    "mean_bce = np.mean(bce_arr)\n",
    "sd_bce = np.std(bce_arr)\n",
    "print(f'bce:{mean_bce:.3f}±{sd_bce:.3f}')\n",
    "\n",
    "acc_arr = np.array(acc_list)\n",
    "acc_mean= np.mean(acc_arr)\n",
    "acc_sd = np.std(acc_arr)\n",
    "print(f'acc:{acc_mean:.3f}±{acc_sd:.3f}')\n",
    "\n",
    "f1_arr = np.array(f1_list)\n",
    "f1_mean= np.mean(f1_arr)\n",
    "f1_sd = np.std(f1_arr)\n",
    "print(f'f1: {f1_mean:.3f}±{f1_sd:.3f}')\n",
    "\n",
    "roc_auc_arr = np.array(roc_auc_list)\n",
    "roc_auc_mean= np.mean(roc_auc_arr)\n",
    "roc_auc_sd = np.std(roc_auc_arr)\n",
    "print(f'roc_auc: {roc_auc_mean:.3f}±{roc_auc_sd:.3f}')\n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adba4fb230ed7dc63d790cdaaae75000cb531237ea070291c76da314f993704f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
